### 《GSSA: A Resource Allocation Scheme Customized for 3D NAND SSDs》

#### 1. 发现的问题

3D NAND SSD 的高存储密度导致写入延迟增加，厂商引入**全序列编程（FSP）操作**（可同时编程多位数据，提升吞吐量），但存在 “性能 - 寿命困境”：
- **性能优势**：FSP 能将 QLC 单元的写入吞吐量提升 4 倍。
- **寿命问题**：FSP 的大粒度编程（如 64KB）会导致小写入被放大（填充空数据），增加无效数据量，加剧垃圾回收（GC）操作，缩短 SSD 寿命。
- 现有解决方案局限：DRAM 缓冲区因 “sync” 命令无法积累足够数据；引入其他 NVM（如 2D NAND）成本过高（是 3D NAND 的 16 倍）。
#### 2. 发现问题的方式

- **实验数据**：通过固定大小随机写入负载测试，发现大粒度编程（如 MPP+FSP）在小写入（16K）场景下吞吐量低（因 GC 频繁），小粒度编程（如基线）在大写入（1M）场景下吞吐量差（图 3a、3b）。
- **sync 命令影响**：分析发现 “sync” 命令强制刷新 DRAM 数据，导致无法积累到 FSP 粒度，小写入放大问题无法缓解（图 4）。
- **写入大小分布**：16 种真实负载中，大量小写入（<16KB）仍需直接写入 NAND，放大问题显著（图 6）。

#### 3. 提出的方案：GSSA（Generalized and Specialized Scramble Allocation）

- **核心思想**：结合 3D NAND 的多种编程操作（基线、MPP、FSP）和架构特性，动态选择编程粒度，平衡性能与寿命。
- **关键设计**：
    - **通用随机分配（GSA）**：为非标准粒度（如 3、5 页）准备专用块，通过 “补充写入”（如 1 页写入）适配 FSP 操作，减少空数据。
    - **专用随机分配（SSA）**：支持非对称 MPP 操作，直接处理非标准粒度写入（如 3 页），最小化空数据。
- **效果**：吞吐量比最小粒度方案高 3.7 倍，比最大粒度方案高 1.04 倍；寿命消耗仅比最小粒度方案高 0.2%，比最大粒度方案低 72%。

### 《RDA: A Read-Request Driven Adaptive Allocation Scheme for Improving SSD Performance》

#### 1. 发现的问题

3D NAND SSD 的高并行分布策略（将连续页分配到多个并行单元）存在缺陷：

- **数据碎片化加剧**：导致 GC 时迁移数据量增加，延迟升高。
- **GC 对延迟影响放大**：读请求访问多个平面时，任一平面处于 GC 状态都会阻塞请求，阻塞概率随并行度增加而上升。
- **冗余并行分布**：多数读请求为小请求（88% 的工作负载中 80% 读请求 < 16KB），高并行分布对其无意义，反而加剧性能损耗。

#### 2. 发现问题的方式

- **并行分布与 GC 关联**：对比 “默认并行度” 和 “低并行度（2）”，发现高并行度下 GC 阻塞的读请求显著更多（图 2）。
- **读请求特性**：25 个真实负载中，小读请求占比极高（<16KB 占 80% 以上），且 42.38% 的读请求仅访问写数据的一部分（“Write> Read” 场景），高并行分布冗余（图 3、4）。

#### 3. 提出的方案：RDA（Read-Request Driven Adaptive Allocation）

- **核心思想**：基于历史读请求大小动态调整写请求的并行度，减少冗余分布，同时保证读并行性。
- **关键设计**：
    - **并行度更新单元**：维护并行度映射表，根据最近 10 个读请求的平均大小调整写请求范围的并行度（如 32-48KB 范围从 3 并行度降至 2）。
    - **数据分配模块**：按并行度和页距离分配平面地址，确保任意 P 个连续页可并行读取（公式：\(X_{plane} = F_{plane} + (X_{lpn} - F_{lpn}) \mod P\)）。
- **效果**：GC 阻塞读请求减少 20.6%，GC 次数减少 7.8%，读响应时间减少 15.8%。

### 三、《The Static Allocation is Not a Static: Optimizing SSD Address Allocation Through Boosting Static Policy》

#### 1. 发现的问题

静态地址分配策略（如 CWDP）存在适应性不足：

  

- **CWDP（通道优先）**：最大化并行性，但在突发 I/O 下因资源冲突导致延迟升高（子请求排队时间长）。
- **PCWD（平面优先）**：减少冲突，但并行性低，轻负载下性能差。
- 单一静态策略无法适配云环境中 “轻负载为主、突发 I/O 集中” 的特性（突发 I/O 占流量大但持续时间短）。

#### 2. 发现问题的方式

- **静态策略对比**：通过随机读写测试，发现 CWDP 在轻负载（I/O 深度 <64）性能优，PCWD 在重负载（I/O 深度> 256）性能更优（图 6）。
- **云负载特性**：分析 AliCloud、TenCloud 等 traces，发现 60%-70% 的重复读写在 5 分钟内，且突发 I/O 流量占比高（>60%）但持续时间短（<1%）（图 2、4）。
- **队列模型分析**：CWDP 在重负载下队列长度（λ）更大，导致子请求等待时间更长（图 8、9）。

#### 3. 提出的方案：HsaP（Hybrid Static Address Allocation Policy）

- **核心思想**：融合 CWDP 和 PCWD 的优势，动态选择策略，平衡并行性与冲突。
- **关键设计**：
    - **策略选择模块**：基于强化学习，轻负载用 CWDP，重负载通过概率（aw）选择 CWDP/PCWD，动态调整 aw 以优化性能。
    - **平面重分配**：每个平面维护两个活动块（对齐块和自由块），满足多平面并行的严格约束。
    - **数据重写**：对 PCWD 写入的高频读数据，按 CWDP 重写以提升读并行性。
- **效果**：平均读写延迟比 CWDP 降低 32.7%-43.7%，多平面操作占比提升 26.2 倍，写放大率最低。
### LiveSSD: A Low-Interference RAID Scheme for Hardware Virtualized SSDs
#### 一、发现的核心问题

在硬件虚拟化 SSD 中，**RAID 技术与性能隔离存在根本性冲突**，主要体现在以下两方面：

  

1. **RAID 方案导致的 I/O 干扰**  
    硬件虚拟化通过为每个租户分配独立的闪存裸片（die）实现性能隔离，但传统 RAID 方案（如 RAID-4/5）会打破这种隔离：
    
    - **RAID-4**：专用奇偶校验裸片会成为性能瓶颈，频繁的奇偶更新导致租户间性能耦合。
    - **RAID-5**：奇偶校验分布在多个裸片，一个租户的写入会触发其他租户裸片的奇偶更新，引入严重 I/O 干扰。
    - **超级页（superpage）基 RAID**：要求多个裸片保持相同写入偏移，与硬件虚拟化中租户独立写入（偏移不同）的特性冲突。
2. **GC 过程中的额外开销**  
    即使采用 NVRAM 存储奇偶校验以避免瓶颈，垃圾回收（GC）时仍需读取无效页来更新奇偶校验，导致额外闪存读取，加剧租户性能波动（平均增加 21.9% 的 I/O 响应时间，最高达 43.7%）。
    

#### 二、发现问题的方式

1. **现有方案的矛盾分析**  
    通过对比逻辑分组 RAID（LG-RAID）和物理分组 RAID（PG-RAID）的缺陷：
    
    - LG-RAID 在硬件虚拟化中会导致租户性能耦合或奇偶瓶颈（图 1a）。
    - PG-RAID 依赖超级页同步写入，与租户独立写入冲突（图 1b）。
2. **实验验证与数据支撑**
    
    - 设计 NV-PG-RAID-4 方案（用 NVRAM 存储奇偶校验），模拟发现其在 GC 时因读取无效页，导致租户平均 I/O 响应时间增加 21.9%，其中连续写入场景（如 w_100_seq）性能下降达 43.7%（图 3、图 6）。
    - 分析真实工作负载（如 MSR-FIU、VDI traces），发现写入密集型和连续写入场景中，无效页聚集导致 GC 时额外读取开销更大（图 8）。

#### 三、解决方案：LiveSSD 方案

#### 核心设计目标

在硬件虚拟化 SSD 中实现 RAID 数据保护，同时最小化奇偶更新导致的 I/O 干扰，保证性能隔离。

#### 关键技术

1. **基于 NVRAM 的奇偶存储**
    
    - 采用 RAID-4 架构，用高速 NVRAM（如 PCM）存储奇偶校验，避免奇偶裸片成为瓶颈。
    - 同一偏移的闪存页和 NVRAM 奇偶页构成条带（stripe），支持租户独立写入（无需同步偏移）。
2. **租户级奇偶更新合并**
    
    - 为每个租户划分 “子超级页 / 子超级块”（同一租户内相同偏移的页），写入和 GC 以子超级页为单位，减少奇偶更新次数（每子超级页更新 1 次奇偶，而非每页）。
3. **主动奇偶更新技术**
    
    - **利用部分页更新**：租户更新部分页时，提前读取旧页并从奇偶保护中移除，避免 GC 时重读（图 5）。
    - **利用空闲时间**：通过指数平滑法预测租户 I/O 空闲时间，在空闲时主动读取无效页并更新奇偶，减少 GC 时的额外开销（算法 1）。
4. **可靠性保障**
    
    - 支持多奇偶校验（如 RAID-6），通过 NVRAM 冗余容忍 NVRAM 裸片故障。
    - 数据一致性通过电容备份电源和元数据定期刷新保证。

### 四、效果

- **性能隔离**：相比 NV-PG-RAID-4，LiveSSD 平均减少 16.3% 的 I/O 响应时间，尾延迟（P99）减少 7.6%，接近无 RAID 的理想状态（Optimal-SSD）。
- **适用性**：在合成负载、真实数据中心和移动设备负载中均有效，尤其在连续写入和高 I/O 强度场景表现优异（图 6、图 7）。

### FlashBlox 论文：问题、发现方式与解决方案

#### 一、作者发现的核心问题

在 SSD 硬件虚拟化场景中，**“性能隔离” 与 “磨损均衡” 存在根本性矛盾**，这是作者发现的核心痛点，具体拆解为以下两方面：

##### 1. 现有虚拟化方案的性能隔离缺陷

随着云计算中多租户共享 SSD 成为常态（如数据库服务、云存储），传统 SSD 虚拟化方案难以实现有效性能隔离：

- **软件隔离方案的局限性**：通过令牌桶限流、权重公平调度等软件手段分配 SSD 资源（如带宽、IOPS），但租户仍共享底层闪存通道（channel）、裸片（die）等硬件资源。例如，一个租户的垃圾回收（GC）操作会占用通道带宽，导致其他租户的 I/O 延迟增加，99 百分位尾延迟最高可恶化 2.6 倍（图 1c、图 9d）。
- **硬件隔离的潜在冲突**：若为租户分配独立的通道或裸片以实现强隔离（如通道级隔离），虽能减少干扰（通道隔离使尾延迟降低 1.7-3.1 倍），但不同租户的写入速率差异会导致硬件资源磨损不均。例如，TPC-C 数据库租户对应的通道磨损速率是 TPCE 租户的 12 倍（图 5），部分通道会提前耗尽擦写（P/E）寿命，导致 SSD 整体寿命缩短 40% 以上（图 14a）。

##### 2. 磨损均衡机制与隔离目标的冲突

传统 SSD 的磨损均衡机制（如块级迁移）是全局调度，与硬件虚拟化的隔离目标相悖：

- **全局磨损均衡破坏隔离**：为平衡各区域磨损，控制器会跨租户迁移数据块，导致租户的 I/O 操作被打断，性能波动增加；
- **无磨损均衡的风险**：若放弃全局调度，仅靠租户内部的磨损均衡，会导致通道 / 裸片磨损差异扩大（磨损不均衡系数 ξ=3.1，图 15），部分通道提前失效后，SSD 容量损失超 6%，且无法再创建跨全通道的高吞吐量虚拟 SSD（vSSD），影响数据中心资源调度灵活性。

#### 二、发现问题的方式

作者通过 “方案对比分析 + 实验验证 + 真实 workload 复盘” 的三层方式，系统性发现并量化了上述问题：

##### 1. 现有方案的矛盾分析

通过对比两种主流 SSD 共享方式的缺陷，明确问题根源：

- **逻辑分组（LG-RAID）与物理分组（PG-RAID）的对比**：传统 RAID 方案中，LG-RAID 会导致租户性能耦合（如 RAID-5 的奇偶更新跨租户），PG-RAID 依赖超级页（superpage）同步写入，与租户独立写入的特性冲突（图 1a、1b）；
- **隔离粒度与性能的关联**：通过拆解 SSD 硬件架构（通道→裸片→平面→块），==发现通道级隔离的干扰最小（通道间仅共享控制器，无总线竞争），裸片级隔离次之（裸片共享通道总线），软件隔离最差（共享所有硬件）==，并通过实验验证不同隔离粒度的性能差异（图 8、图 9）。

##### 2. 实验验证与数据支撑

基于开源通道 SSD（CNEX 1TB SSD）搭建原型，通过控制变量法量化问题：


- **性能隔离实验**：在软件隔离、裸片隔离、通道隔离三种模式下，运行 YCSB（键值存储）、TPC（数据库）等 14 种 workload（表 4），发现通道隔离的吞吐量比软件隔离高 1.6 倍，99 百分位尾延迟低 2.6-3.1 倍（图 8、图 10）；
- **磨损不均实验**：模拟 “极端写入” 场景（如单个通道承担所有写入，其他通道仅读取），发现无磨损均衡时 SSD 寿命仅 4 个月，而理想磨损均衡下寿命可达 5 年以上（图 14a）；
- **真实 workload 分析**：基于微软数据中心的云存储、Web 搜索、MapReduce 等 trace（表 4），发现不同 workload 的块擦除速率差异达 10 倍以上（图 5），进一步验证磨损不均的必然性。

##### 3. 真实场景需求复盘

结合云服务的实际需求，发现现有方案的适配缺陷：

- **SLO（服务等级目标）冲突**：云数据库（如 Azure DocumentDB）需保证 250-2500 QPS 的稳定延迟，而软件隔离下的性能波动会导致 SLO 违约率增加 30%；
- **资源利用率矛盾**：数据中心 SSD 容量通常达 20TB 以上，而租户需求多为 10GB-1TB，需虚拟化切片，但传统方案要么牺牲隔离（软件），要么牺牲寿命（硬件），无法兼顾。

#### 三、解决方案：FlashBlox 方案

针对 “性能隔离与磨损均衡” 的矛盾，FlashBlox 提出 “分层隔离 + 粗粒度磨损均衡” 的双层设计，核心目标是在保证强隔离的同时，实现接近理想的 SSD 寿命。

##### 1. 核心架构：多级隔离的 vSSD 模型

FlashBlox 将 SSD 硬件资源（通道、裸片、平面）抽象为不同隔离等级的 vSSD，租户按需选择，底层通过资源管理器实现灵活分配（图 3）：

|vSSD 类型|隔离等级|分配粒度|适用场景|性能特点|
|---|---|---|---|---|
|通道隔离 vSSD|高|通道|延迟敏感型（如 Web 搜索、高频交易数据库）|无总线竞争，99 百分位延迟降低 3.1 倍|
|裸片隔离 vSSD|中|裸片|吞吐量敏感型（如 MapReduce）|共享通道总线，延迟比通道隔离高 1.2 倍|
|软件隔离 vSSD|低|软平面（soft-plane）|低成本场景（如备份存储）|依赖令牌桶限流，延迟比硬件隔离高 2.6 倍|


- **关键设计**：每个 vSSD 的超级块（superblock）仅由其分配的硬件资源构成（如通道隔离 vSSD 的超级块仅跨自身通道），避免跨租户资源占用；同时支持 vSSD 动态扩容（如增加通道数量提升吞吐量），适配云服务的弹性需求。

##### 2. 磨损均衡：粗粒度迁移 + 自适应调度

为解决硬件隔离导致的磨损不均，FlashBlox 设计 “跨 vSSD 粗粒度迁移 + intra-vSSD 细粒度均衡” 的双层机制：

##### （1）跨 vSSD 粗粒度迁移：平衡磨损且最小化干扰

- **迁移策略**：定期（自适应调整周期）将磨损最严重的通道 / 裸片，与磨损速率最低的通道 / 裸片进行 “块级交换”，迁移时仅暂停涉及的块的 I/O，其他块正常处理（图 3、图 12）；
- **周期计算**：基于数学模型推导最小迁移周期，确保磨损不均衡系数 ξ≤1.1（即最大磨损不超过平均磨损的 10%）。例如，16 通道 SSD 在 δ=0.1（允许 10% 不均衡）时，迁移周期约 12 天（表 2），远低于 SSD 的 150-250 周设计寿命；
- **迁移优化**：利用租户 I/O 空闲时间（如 Web 搜索的 15.73ms 平均空闲间隔，表 2）执行迁移，单次迁移 1GB 数据仅增加 34.2% 的延迟（图 13），且迁移频率低（真实 workload 下平均每 94 天一次，表 5），对租户性能影响可忽略（长期平均性能损失 0.04%）。

##### （2）intra-vSSD 细粒度均衡：保证租户内部磨损均匀

- **块级映射表**：每个 vSSD 维护独立的 “逻辑块→物理块” 映射表（8MB/TB 开销），通过块迁移平衡自身硬件资源的磨损；
- **与应用层协同**：FlashBlox 提供 API（表 3），支持应用层（如 LevelDB、vLFS 文件系统）按块粒度管理数据，避免传统 FTL 的多层地址转换开销，同时确保 GC 操作仅在 vSSD 内部执行，不干扰其他租户。

##### 3. 实现细节：低开销与高可靠性保障

- **硬件层面**：基于开源通道 SSD（CNEX 1TB，16 通道 ×4 裸片 / 通道）实现，修改固件以支持通道 / 裸片的独立调度，代码量 11219 行（C 语言）；
- **数据一致性**：迁移时通过电容备份电源保证数据持久化，映射表损坏时可通过块元数据（16 字节 / 块，含逻辑地址、擦除计数器）重建；
- **过载保护**：针对恶意租户（如持续高写入的 “通道杀手”），通过监控擦除速率触发迁移，避免单一租户耗尽 SSD 寿命（图 14a 中 FlashBlox 使 SSD 寿命达到理想值的 95%）。

##### 4. 效果验证

- **性能隔离**：通道隔离 vSSD 的吞吐量比软件隔离高 1.6 倍，99 百分位尾延迟降低 3.1 倍（图 8、图 10）；
- **磨损均衡**：在 14 种真实 workload 下，磨损不均衡系数 ξ=1.02（接近理想值 1.0，图 15），SSD 寿命达到理想值的 95%（表 5）；
- **资源灵活性**：支持 128 个通道隔离 vSSD 或 1024 个裸片隔离 vSSD，满足数据中心多租户的差异化需求。
### 《An Efficient, QoS-Aware I/O Scheduler for Solid State Drive》核心内容解析

#### 一、文章背景

随着闪存技术的发展，固态硬盘（SSD）凭借低功耗、高 I/O 性能和紧凑尺寸，在大规模存储服务器中逐渐取代机械硬盘（HDD）。例如，美国圣地亚哥超级计算中心（SDSC）构建的 Gordon 闪存系统，依托 64 个闪存 I/O 节点，可实现 275GB/s 的顺序读取和 210GB/s 的顺序写入性能 。然而，SSD 仍存在成本较高、容量相对有限的问题 —— 部署 Gordon 系统需花费 2000 万美元采购 256TB 闪存，这使得为每个特定应用单独构建闪存存储系统的方案不具实用性 。

在此背景下，云存储技术应运而生，通过整合大量 SSD 为各类应用提供共享存储服务，以降低单独部署和管理未充分利用的闪存系统的额外成本。但在共享存储环境中，不同应用可能以自私的方式争夺存储资源，且混合工作负载的存储特性差异、各应用的性能需求不同等因素，易导致存储资源分配不当，违反租户与云服务提供商之间的服务级别协议（SLA） 。因此，云存储系统亟需具备服务质量（QoS）感知能力的 I/O 调度器，为不同应用提供性能隔离，确保各应用获得符合其付费等级的存储服务。

早期针对存储系统的 QoS 调度研究多聚焦于 HDD，典型调度器（如基于网络领域的 WFQ、SFQ 变体）基于 HDD 的机械特性设计，通过电梯式调度（如 SCAN 算法）优化磁头移动成本。但 SSD 基于半导体芯片构建，完全消除了 HDD 的寻道成本，内部采用多通道、多芯片的并行架构，传统 HDD 调度方案无法适配 SSD 的特性 。虽然后续出现了针对 SSD 的调度技术（如读优先、写排序与捆绑、写块优先等），但这些技术多关注提升 SSD 的 I/O 效率，忽视了多应用共享场景下的性能隔离需求；部分方案（如区域调度器）虽尝试利用 SSD 并行性，却未考虑物理页在芯片和通道层面的交错分布特性，难以充分发挥 SSD 的并行优势 。

#### 二、作者发现的核心问题

作者聚焦 SSD 共享存储场景，发现现有 I/O 调度方案在 “性能隔离” 与 “I/O 效率” 的平衡上存在两大关键问题：

##### 1. 问题 1：无法精准量化 SSD 中读写请求的成本，导致性能隔离失效

HDD 中请求以独占方式执行，请求延迟可直接作为服务时间衡量资源消耗；但 SSD 采用并行架构，多个请求可同时在不同通道、芯片上处理，且单个请求可能跨芯片执行，使得 “请求延迟≠资源消耗”，传统基于延迟的成本模型完全失效 。更关键的是，SSD 的读写性能差异显著（读性能远优于写性能），且写请求的成本受工作负载特性影响动态变化（如随机小写会触发高频垃圾回收，增加额外开销），但现有调度方案（如 WFQ）未区分读写请求的成本，将相同权重分配给不同类型请求 —— 例如，为读请求和写请求分配相同的时间预算，导致写请求因实际消耗更高而占用过多资源，读密集型应用的性能被严重挤压，无法实现性能隔离 。

##### 2. 问题 2：未充分利用 SSD 的并行架构，I/O 效率受限

SSD 通过多通道、多芯片的并行设计提升性能，数据通常以条带化（RAID-like）方式分布在不同通道以挖掘并行性。但现有针对 SSD 的调度方案（如 FIOS）多将 SSD 视为 “黑盒”，不感知其内部通道结构，仅在请求队列层面做优先级调度（如读优先），未根据请求的物理地址将其分配到对应的通道队列，导致不同通道的负载不均衡，无法充分利用 SSD 的通道级并行性，I/O 效率难以达到最优 。此外，部分方案虽尝试利用并行性（如区域调度器），但未考虑物理页在通道和芯片层面的交错分布，条带化数据的调度粒度与通道结构不匹配，并行优势发挥受限 。

#### 三、发现问题的方式

作者通过 “现有方案对比分析 + SSD 特性拆解 + 实验验证” 的三层方法，系统性发现并量化问题：

##### 1. 现有方案的矛盾分析

对比不同调度方案的设计逻辑与适配场景，明确问题根源：

- **HDD 调度方案 vs SSD 特性**：HDD 调度方案（如 WFQ、CFQ）基于机械特性设计，依赖电梯式调度优化寻道成本，但 SSD 无寻道成本，且并行架构与 HDD 完全不同，这些方案在 SSD 上不仅无优势，还会因未区分读写成本导致性能隔离失效 ；
- **SSD 调度方案的局限性**：现有 SSD 调度技术（如 FIOS、写捆绑）虽考虑了 SSD 的读写差异，但多将 SSD 视为黑盒，不感知内部通道结构，无法充分利用并行性；部分方案（如区域调度器）虽尝试利用并行性，却未适配物理页的条带化分布特性，调度粒度不合理 ；
- **成本模型合理性验证**：通过分析 SSD 的并行执行机制 —— 多通道可同时处理请求，且读写请求的资源消耗差异显著（写请求需擦除前写入，成本是读请求的数倍），证明传统 “统一成本模型”（如将读写请求视为同等成本）无法反映真实资源消耗，必然导致性能隔离失效 。

##### 2. 实验验证与数据支撑

基于 DiskSim 模拟器（带 MSR SSD 扩展模块）构建实验环境，通过对照实验暴露问题：

- **WFQ 调度的缺陷验证**：在 “读密集应用 + 动态改变读写比例的应用” 共享场景中，WFQ 基于固定权重分配资源，未区分读写成本。当应用 2 的写请求比例从 0% 提升至 100% 时，应用 1（读密集）的延迟从约 1ms 升至 3ms，性能被严重干扰，证明 WFQ 无法实现性能隔离 ；
- **现有 SSD 调度方案的效率瓶颈**：对比 FIOS 与 “无通道队列的 BCQ”（逻辑类似 FIOS），发现两者均未利用 SSD 通道并行性。在混合读写工作负载下，应用 1（读密集）的吞吐量波动达 20%，且整体吞吐量比充分利用通道并行性的方案低约 20%，验证了未感知通道结构的方案存在 I/O 效率瓶颈 。

#### 四、解决方案：两级 QoS 感知 I/O 调度框架（BCQ）

针对上述问题，作者提出 “预算分配（Budget Allocation）+ 通道队列（Channel-based Queuing）” 的两级调度框架（BCQ），在保证性能隔离的同时，充分挖掘 SSD 的并行优势提升 I/O 效率，具体设计如下：

##### 1. 整体架构

BCQ 将 “服务分配（性能隔离）” 与 “性能增强（I/O 效率）” 分离，构建两级调度架构（如图 1 所示）：
  
- **高层调度器**：为每个应用维护专属 FIFO 队列，基于服务时间管理预算，核心目标是实现性能隔离；
- **低层调度器**：根据 SSD 的通道结构构建通道级队列，将高层调度器分发的请求按物理地址分配到对应通道队列，目标是利用 SSD 的通道级并行性提升 I/O 效率 。

##### 2. 高层调度器：基于动态成本估算的预算分配

###### （1）预算分配机制

- **预算定义**：以 “服务时间” 作为预算单位，而非传统的吞吐量（如 IOPS、MB/s），避免因工作负载特性波动导致的资源分配不准确。根据应用的服务比例$(p_i)$，由 SLA 确定）和预算补充周期$(T_{period})$，默认 100ms），为每个应用$(a_i)$分配预算：$(B_i = T_{period} × p_i)$ ；
- **预算补充触发条件**：当所有应用耗尽预算、预算周期结束、新应用激活或所有有剩余预算的应用空闲时，触发预算补充，确保资源不闲置且新应用能及时获得服务 。

###### （2）动态读写成本估算

- **初始离线校准**：在 SSD 安装时，通过离线测试初步获取读写请求的平均成本 —— 向 SSD 发送纯读或纯写请求，假设同类型请求成本相同，以 “总耗时 / 请求数” 计算初始平均读成本$Cost_r$和写成本$Cost_w$ ；
- **运行时线性回归优化**：初始校准成本可能因工作负载变化、队列长度波动存在误差。运行时通过滑动窗口（记录最近 20 个观测点）收集统计数据，设窗口时间z内完成x个读请求和y个写请求，建立线性模型：$(z = Cost_r × x + Cost_w × y + \varepsilon)$（$(\varepsilon)$为随机误差，服从正态分布）。采用最大似然估计求解正规方程$(\hat{C} = (X^T X)^{-1} X^T Z)$（其中X为读写请求数矩阵，Z为时间窗口矩阵，C为成本向量），周期性更新读写成本，确保成本估算的准确性 ；
- **预算扣除规则**：当请求从应用队列分发到低层时，根据请求类型（读 / 写），从对应应用的预算中扣除估算的成本（读请求扣$(Cost_r)$，写请求扣$(Cost_w)$），若预算耗尽，该应用的后续请求需等待下一轮预算补充 。

##### 3. 低层调度器：基于通道结构的队列管理

###### （1）通道队列构建

BCQ 将 SSD 视为 “灰盒”，需感知其内部关键参数（如条带大小S、通道数N）。根据 SSD 的条带化寻址特性，计算 “簇页大小”$(F = S / N)$（每个通道负责的条带数据大小）。对于每个请求，根据其物理起始地址H，通过公式$(I = (round(H / F)) mod N)$确定对应的通道队列，将请求分配到该队列；若请求大小超过F，则拆分为多个子请求分别分配到对应通道队列 。

###### （2）通道内请求调度优化

- **读优先调度**：在每个通道队列内部，给予读请求更高优先级，避免写请求（尤其是耗时的垃圾回收相关写操作）阻塞读请求，缓解 “读被写阻塞” 的干扰 ；
- **写排序与捆绑**：对通道队列内的写请求按物理地址排序，并捆绑成与簇页大小匹配的批次发送到 SSD，减少写操作的次数和碎片，提升写性能（实验表明，捆绑写可使写吞吐量提升约 15%） ；
- **并行分发**：不同通道队列的请求可同时向对应通道分发，充分利用 SSD 的通道级并行性，最大化 I/O 效率 。

##### 4. 方案效果验证

通过合成工作负载和真实工作负载（WebSearch 搜索引擎、FinTrans 金融交易 OLTP 应用）验证 BCQ 的有效性：

- **性能隔离**：在 “读密集应用 + 动态改变读写比例的应用” 场景中，BCQ 能确保读密集应用的延迟稳定在 1ms 以内（波动小于 5%），而 WFQ 下该应用延迟波动达 200%；当多应用动态加入 / 退出时，BCQ 可根据服务比例调整预算，使各应用吞吐量按比例分配（如两应用服务比例 1:1 时，读应用吞吐量约 80MB/s，写应用约 40MB/s，符合读写成本差异） ；
- **I/O 效率**：对比 FIOS、无通道队列的 BCQ，BCQ 在合成工作负载下整体吞吐量提升约 20%（峰值达 120MB/s，而其他方案约 100MB/s）；在真实工作负载下，WebSearch 应用的平均延迟控制在 300ms 以内，远低于 FIOS 的 4000ms 和无通道队列 BCQ 的 1200ms，且队列长度更短，资源利用率更高 。

#### 五、总结

BCQ 调度框架通过 “高层动态成本预算” 解决了 SSD 中读写请求成本量化难的问题，实现多应用的精准性能隔离；通过 “低层通道队列” 充分利用 SSD 的并行架构，提升 I/O 效率。该方案的核心价值在于：首次明确将 SSD 视为 “灰盒”，结合其物理结构（通道、条带化）和动态成本特性，实现 “性能隔离” 与 “I/O 效率” 的双重目标，为云存储环境下的 SSD 共享调度提供了可落地的解决方案。后续研究可进一步探索将该框架扩展到多 SSD 集群场景，或结合磨损均衡机制优化 SSD 的生命周期管理。
### 《WA-OPShare: Workload-Adaptive Over-Provisioning Space Allocation for Multi-Tenant SSDs》核心内容解析

#### 一、文章背景

##### 1. 技术与应用趋势

随着 3D 堆叠、每单元多比特等 NAND 闪存技术发展，SSD 凭借大容量（可达数 TB）、高带宽（可达数 GB/s）优势，广泛应用于云、企业及移动存储系统。多租户共享单块 SSD 成为主流实践（如 4TB SSD 承载多个数十至数百 GB 的数据库实例），以提升存储利用率、降低单独部署成本（🔶5-9）。

##### 2. 核心资源矛盾：OPS 的关键作用与分配挑战

- **OPS 定义与价值**：过配置空间（OPS）是用户不可见的额外存储空间（占容量 7%-40%，典型值 25%），可优化垃圾回收（GC）效率 ——OPS 越大，无效页容纳量越多，GC 有效页迁移量越少，写放大（WAF）越低，进而提升 SSD 性能与寿命（🔶5-11、🔶5-29）。
- **租户收益差异**：不同租户对 OPS 收益显著不同，随机写密集租户（如文件服务器）收益高，读密集租户（如 Aerospike 数据库）需求低，易出现 “OPS 闲置” 与 “OPS 短缺” 并存（🔶5-11、🔶5-48）。

##### 3. 现有方案缺陷

|方案类型|核心逻辑|缺陷|
|---|---|---|
|静态分区（Partition）|将 OPS 静态划分为超块组，租户独占一组|无法适配 workload 动态变化，性能损失可达 21%（🔶5-41）|
|自由共享（Sharing）|租户共享全局 OPS 池，全局 GC 回收|缺乏分配引导，WAF 高的租户抢占低 WAF 租户 OPS，整体带宽比 Partition 低 20.6%（🔶5-46）|
|OPS-Isolation|动态分区|仅以 “带宽公平” 为目标，未考虑租户收益差异，无法最大化 OPS 价值（🔶5-35）|

#### 二、作者发现的核心问题

##### 1. 问题 1：OPS 存在 “隐性闲置”，空间利用率低

SSD 中存在 “惰性超块（lazy superblock）”—— 含大量 “长期无效页”（已失效但未被 GC 擦除的页），这些页形成 “僵尸时间（Zombie Time，失效到擦除的时间）”，占用 OPS 空间却未利用。传统方案仅依赖 GC 被动回收，OPS 利用率低于 50%，GC 效率低、写放大升高（🔶5-74、🔶5-75）。

##### 2. 问题 2：OPS 分配未匹配租户收益差异，整体性能次优

- **收益差异来源**：写模式（随机写收益高）、读写比（写占比越高收益越高）、当前 OPS 规模（OPS 越少，额外 OPS 收益越高）。例如，随机写租户（fileserver_w）1GB 额外 OPS 可提升带宽 12.5MB/s，读密集租户（w-0%）不足 1MB/s（🔶5-48、🔶5-57）。
- **方案缺陷**：Partition 按固定比例分配，无法倾斜高收益租户；Sharing 以 “WAF 趋同” 为目标，整体性能比最优分配低 40.3%（🔶5-41、🔶5-111）；且缺乏 runtime 收益预测，无法适配 workload 动态变化（🔶5-59）。

#### 三、发现问题的方式

##### 1. 现有方案对比分析

- **静态分区刚性验证**：“webproxy（读密集）-fileserver（写密集）” 组合中，最优 OPS 分配（20%:80%）与最差（90%:10%）带宽差异达 40.3%，证明静态分配无法适配 workload（🔶5-41、🔶5-111）。
- **自由共享竞争问题**：“webproxy-varmail” 实验中，Sharing 方案下 WAF 高的 varmail 抢占 webproxy 的 OPS，webproxy OPS 从 5.3GB 降至 2.1GB，带宽下降 18%（🔶5-46、🔶5-47）。

##### 2. 实验验证与数据支撑

- **隐性闲置影响**：四租户场景中，惰性超块无效页占 OPS 的 35%，GC 有效页迁移量比理想状态多 40%，写放大升高 29%（🔶5-74、🔶5-126）。
- **动态 workload 适配缺陷**：租户 workload 每 150 分钟切换时，Partition 带宽波动 15%，Sharing 整体带宽比最优低 31.2%（🔶5-146、🔶5-111）。

##### 3. workload 特性拆解

通过实验明确 OPS 收益影响因素：

- 写模式：随机写无效页生成率是顺序写的 3-5 倍；
- 读写比：写占比 0%→100% 时，边际增益 1MB/s→12.5MB/s；
- 当前 OPS 规模：OPS 1GB 时增益 12.5MB/s，5GB 时降至 2.5MB/s（🔶5-48、🔶5-57、🔶5-58）。

#### 四、解决方案：WA-OPShare 动态 OPS 分配方案

##### 1. 整体架构

包含两大核心模块，基于预算型 I/O 调度器（按租户权重分配服务时间），为租户维护私有超块池避免 GC 干扰（🔶5-65、🔶5-71）：

- **惰性超块压缩模块**：识别并压缩惰性超块，释放隐性闲置 OPS；
- **OPS 分配器模块**：通过 “测量 - 重分配” 双窗口，量化租户收益并动态分配 OPS。

##### 2. 核心技术 1：空间效率模型（释放隐性闲置 OPS）

###### （1）关键定义与公式

- **僵尸时间**：页从失效到擦除的时间，反映无效页闲置时长；
- **惰性（Laziness）**：超块内所有页僵尸时间之和，即$$(Laziness_i = \sum_{j=1}^N ZombieTime_{i,j})$$；
- -**空间效率公式**（独立公式块格式）： $$Efficiency_i = \frac{Ratio_{valid,i}}{Laziness_i} = \frac{N_{valid,i}}{N \times \sum_{j=1}^N ZombieTime_{i,j}} $$其中，$Ratio_{valid,i}$ 为超块i有效页比例，$N_{valid,i}$为有效页数量，N为超块总页数（🔶5-76、🔶5-77）。

###### （2）压缩流程

1. **轻量级元数据维护**：为每个超块维护$T_{written,i}$（写满时间）和$ALT_i$（累计生命周期），间接计算僵尸时间：$$ (\sum_{j=1}^N ZombieTime_{i,j} = N \times (T_{cur,i} - T_{written,i}) - ALT_i)$$ 40GB SSD 仅需 20kB 额外 DRAM；
2. **定期压缩**：每 60 秒扫描超块，选择效率最低的 N 个惰性超块，将有效页迁移到 N-1 个超块，释放 1 个空闲超块；
3. **带宽控制**：压缩带宽限制在 SSD 总带宽的 1% 以内。

##### 3. 核心技术 2：OPS 边际增益模型（量化收益并分配）

###### （1）测量窗口：量化 OPS 收益（Δx）

- 记录窗口开始$(WAF_{begin})$与结束$(WAF_{end})$时的 WAF，及超块数量（m），计算： $\Delta x = \frac{WAF_{begin} - WAF_{end}}{m}$ ，Δx 越大，租户 OPS 收益越高（🔶5-88、🔶5-89）。

###### （2）重分配窗口：收益优先分配

基于 Δx，通过边际增益公式$$(MG = Bandwidth(WAF-\Delta x) - Bandwidth(WAF))$$预测收益，将释放的超块分配给 MG 最大的租户；若租户隔离等级 Iso=1，超块返还原租户（🔶5-85、🔶5-90）。

##### 4. 方案效果

|对比维度|性能提升（vs Partition）|性能提升（vs Sharing）|写放大降低（vs Partition）|写放大降低（vs Sharing）|
|---|---|---|---|---|
|最优场景|40.3%|31.2%|37.0%|17.5%|
|八租户场景|8.9%（整体带宽）|-|10.1%|-|
|写密集租户|25.0%-75.4%（带宽）|-|-|-|
|（🔶5-111、🔶5-129、🔶5-140）|||||
### 《Dynamic Load Balancing of Dispatch Scheduling for Solid State Disks》核心问题与解决方案解析

#### 一、论文发现的核心问题

作者聚焦 SSD 在服务器多应用共享场景中的调度缺陷，发现现有方案无法同时解决 “芯片冲突”“读写干扰” 与 “长尾延迟” 三大核心问题，具体拆解如下：

##### 1. 问题 1：静态条带化导致 “芯片冲突”，并行性利用率低

SSD 采用类似 RAID-0 的静态条带化架构，通过地址模运算（`(s^i + n) mod P`，`P`为闪存芯片总数）将数据分配到固定芯片，但服务器 workload 的小尺寸、随机访问特性导致：


- **局部芯片过载**：服务器应用的 I/O 请求多为 4-32KB（如 Web 服务器平均 11KB、数据库 22KB），远小于 SSD 的条带化单元（如 16 颗芯片的条带化单元为 64KB）。小尺寸请求仅访问 1-2 颗芯片，若多个请求映射到同一芯片，需串行执行，产生 “芯片冲突”。例如，2 个 4KB 请求均映射到芯片 0 时，需先后执行，后发请求延迟从 50μs 升至 750μs，延迟升高 14 倍 ；
- **全局资源浪费**：冲突发生时，仅少数芯片忙碌（如 16 颗芯片中仅 2 颗被访问），其他芯片空闲，空闲时间占比达 35%，SSD 整体并行性利用率不足 60% ；
- **大尺寸请求的局限性**：即使请求尺寸较大（如 64KB），虽能访问所有芯片，但传统调度未考虑芯片负载差异，若部分芯片因 GC 或磨损均衡处于忙碌状态，仍会导致请求延迟升高 。

##### 2. 问题 2：读写延迟差异引发 “读写干扰”，读性能被严重挤压

SSD 的写操作延迟（约 700μs）是读操作（约 50μs）的 14 倍，且写操作需额外执行垃圾回收（GC）、磨损均衡等内部操作，现有调度方案的读写处理策略存在明显缺陷：


- **静态批处理适配性差**：部分方案（如 ParDispatcher）采用编译时固定的批大小（读 32 个 / 批、写 16 个 / 批），若 workload 的读写比与批大小不匹配（如 Web 服务器读写比 9:1，而批大小比 4:1），会导致读请求排队阻塞，读吞吐量下降 7-10% ；
- **混合调度加剧干扰**：若读写请求混合调度，读请求会被写请求阻塞。例如，1 个 4KB 写请求（延迟 700μs）优先调度后，后续 8KB 读请求需等待其完成，读延迟从 100μs 升至 800μs，升高 7 倍 ；
- **缺乏动态反馈**：传统方案（如 FlashFQ）未实时感知 SSD 的读写处理时间变化（如写操作因 GC 延迟从 700μs 升至 1400μs），仍按固定比例调度，导致干扰加剧 。

##### 3. 问题 3：请求重排序导致 “长尾延迟”，QoS 无法保障

为提升吞吐量，部分方案（如 PIQ、BCQ）通过重排序避免芯片冲突，但过度重排序会导致早到达的请求被长期延迟，产生 “长尾延迟”（99.9% 分位延迟）：

- **重排序优先级失衡**：PIQ 通过 “冲突向量” 筛选无冲突请求，优先调度大尺寸请求以减少冲突，但会导致小尺寸请求（如 4KB）长期排队，99.9% 分位延迟从 1ms 升至 25ms，违反服务等级协议（SLA） ；
- **无延迟约束机制**：现有方案未对重排序次数或等待时间设置限制，若某请求因 “低冲突优先级” 被反复延后，会出现 “请求饥饿”，极端情况下等待时间超 100ms ；
- **未适配动态负载**：当 workload 从读密集转为写密集时，传统方案无法调整重排序策略，仍优先调度读请求，导致写请求堆积，进而反向阻塞读请求，形成 “双向干扰” 。

##### 4. 问题 4：现有方案的单一维度优化局限

现有针对 SSD 的调度方案仅解决单一问题，无法兼顾多目标：

- **仅优化芯片冲突（如 PIQ）**：通过冲突向量筛选无冲突请求，但未处理读写干扰，读性能仍受写操作挤压；
- **仅优化读写分离（如 FIOS）**：采用读优先策略，但未考虑芯片冲突，小尺寸随机读请求仍因冲突延迟升高；
- **仅关注公平性（如 FlashFQ）**：通过公平队列分配带宽，但未优化并行性利用率，吞吐量比理想状态低 30% 。

#### 二、发现问题的方式

作者通过 “架构特性分析 + 实验对比验证 + workload 拆解” 的三层方法，系统性发现并量化问题，具体如下：

##### 1. 架构特性与 workload 冲突分析

拆解 SSD 静态条带化架构与服务器 workload 的不兼容性，明确问题根源：


- **芯片冲突量化实验**：在 16 颗芯片的 128GB SSD 上，运行 4KB 随机读写 workload（读写比 1:1），通过跟踪芯片访问记录发现，约 38% 的请求会触发芯片冲突，冲突请求的平均延迟（800μs）是无冲突请求（500μs）的 1.6 倍，且冲突频率随线程数增加呈线性上升（256 线程时冲突率达 52%） ；
- **读写干扰验证**：构建 “写请求阻塞读请求” 场景，100 个写线程（4KB 随机写）与不同数量的读线程混合运行，发现读线程数量从 32 增至 256 时，NOOP 调度的读吞吐量从 240MB/s 降至 120MB/s，而读写分离调度的读吞吐量仅下降 15%，证明读写干扰是读性能下降的核心原因 ；
- **长尾延迟分析**：在数据库 workload（200 线程、随机访问）中，对比 PIQ 与 NOOP 的延迟分布，PIQ 的 99.9% 分位延迟（25ms）是 NOOP（5ms）的 5 倍，证明过度重排序会导致长尾延迟失控 。

##### 2. 现有方案的对比实验

基于 SSDsim 模拟器与真实 SSD（Samsung PM850 Pro），对比主流调度方案的缺陷，量化问题严重程度：

- **静态批处理的局限**：在 Web 服务器 workload（读写比 9:1）中，ParDispatcher 的静态批大小导致读请求排队，吞吐量比 NOOP 低 7%（128GB SSD）、10%（256GB SSD），且读延迟的 99.9% 分位值升高 2.3 倍 ；
- **冲突避免方案的不足**：PIQ 通过冲突向量筛选无冲突请求，但对小尺寸随机请求的筛选成功率仅 45%，且未处理读写干扰，读延迟比理想状态高 30% ；
- **公平调度的吞吐量损失**：FlashFQ 虽实现公平调度，但未优化芯片冲突，在 256 线程场景下，吞吐量比理论值低 28%，芯片并行性利用率不足 55% 。

##### 3. workload 特性拆解

分析四类典型服务器 workload 的 I/O 特征，明确不同场景下的问题差异：

|workload 类型|平均请求尺寸|读写比|访问模式|核心痛点|
|---|---|---|---|---|
|Web 服务器|11KB|9:1|随机|读请求被写请求阻塞，芯片冲突频繁|
|文件服务器|64KB|1:9|随机|写操作触发 GC，读请求延迟升高|
|数据库（OLTP）|22KB|2:1|随机|芯片冲突与读写干扰并存，吞吐量波动大|
|邮件服务器|16KB|1:1|顺序|读写混合调度导致顺序写性能下降|


通过拆解发现，不同 workload 的核心痛点不同，需动态调整调度策略，传统单一策略无法全覆盖 。

#### 三、解决方案：Dynamic Load Balanced Queuing（DLBQ）

针对上述问题，作者提出 “动态负载均衡队列（DLBQ）” 方案，通过 “虚拟时间成本模型 + 动态队列选择 + 均衡芯片利用率” 的三层设计，实现 “高吞吐量” 与 “有界延迟” 的平衡，具体如下：

##### 1. 核心技术 1：虚拟时间成本模型 —— 实时感知芯片负载

为量化各闪存芯片的动态负载，DLBQ 引入 “虚拟时间” 概念，动态跟踪芯片的忙闲状态，解决传统方案 “黑盒式调度” 的缺陷：

###### （1）芯片访问映射

对于每个 I/O 请求`r^i`（起始地址`s^i`、长度`l^i`），通过地址模运算确定其访问的 “活跃芯片（active chip）”：$$ (c_n \leftarrow (s^i + n) \mod P, \quad 0 \leq n < \min(l^i, P))$$ 例如，16 颗芯片（`P=16`）的 SSD 中，起始地址 13、长度 8KB（2 页）的请求，会访问芯片`(13+0) mod 16=13`与`(13+1) mod 16=14`，仅 2 颗芯片活跃 。

###### （2）虚拟时间计算

为每颗芯片维护 “虚拟开始时间（`S_k`）” 与 “虚拟结束时间（`F_k`）”，分别表示芯片开始空闲的虚拟时间与处理完当前请求的虚拟时间：


- **请求调度时**：将活跃芯片的虚拟结束时间累加 “操作成本$（L_{c_n}^i）$，$L_{c_n}^i$为请求分配给该芯片的页数（按芯片数量平均分配，余数优先分配给前$l^i mod P$颗芯片）；
- **请求完成时**：将活跃芯片的虚拟开始时间更新为当前虚拟结束时间，若芯片因 GC 等内部操作延迟，通过实际完成时间修正虚拟时间（公式 5），确保与真实状态一致，误差率仅 8% 。


该模型能精准反映芯片负载 —— 例如，某芯片处理写请求后因 GC 延迟，虚拟时间会滞后于实际时间，DLBQ 通过反馈机制调整，确保调度决策基于实时状态 。

##### 2. 核心技术 2：动态队列选择（DQS）—— 解决读写干扰

DQS 通过 “动态批处理时间（`BT_y`）” 与 “调度比（`SR_y`）”，实现读写请求的自适应分离，避免读请求被写请求阻塞：

###### （1）调度比计算

根据 workload 的读写比与 SSD 的读写延迟，动态调整读写队列的调度优先级： $$(SR_r(t) \leftarrow \frac{TL_r(t) \times I_w(t)}{TL_w(t) \times I_r(t)})$$ 其中`TL_r`/`TL_w`为读写队列的等待请求总量，`I_r`/`I_w`为读写操作的平均处理时间（`I_r`预定义为读 - only workload 测得的基准值，`I_w`实时测量以适配 GC 等动态开销）。例如，Web 服务器读写比 9:1，`I_r=50μs`，`I_w=700μs`，则`SR_r= (9×700)/(1×50)=126`，读队列获得更高调度优先级 。

###### （2）动态批处理时间

基于芯片的虚拟结束时间与调度比，确定当前队列的激活时长（`BT_y`），避免读写请求混合： $$(BT_y(t) \leftarrow \max_k F_k(t) + NI_{\neg y} \times SR_y(t))$$ 其中$$NI_{\neg y}$$为相反类型队列的前一次非混合调度间隔，确保读写请求在时间上完全分离。例如，读队列激活时，`BT_r`为当前芯片最大虚拟结束时间加上`NI_w×SR_r`，读请求在该时间段内独占芯片，无写请求干扰 。

###### （3）队列切换机制

DQS 通过三个条件动态切换活跃队列，平衡读写公平性：

1. 活跃队列为空时切换；
2. inactive 队列的延迟比超过调度比（如读队列活跃时，写队列延迟比`WR_w > SR_r`）；
3. 芯片最小虚拟开始时间达到批处理时间（`min S_k ≥ BT`），确保当前队列的请求已充分利用芯片资源 。

实验表明，DQS 可使读吞吐量比 NOOP 提升 13-17%，读写干扰导致的读延迟升高幅度从 52% 降至 8% 。

##### 3. 核心技术 3：均衡芯片利用率（BCU）—— 缓解芯片冲突 + 控制长尾延迟

BCU 通过 “利用率估算 + deficit 时间补偿”，重排序请求以平衡芯片负载，同时避免请求因过度重排序导致的长尾延迟：

###### （1）利用率估算

对每个候选请求，计算其调度后所有芯片的 “总负载（`TC`）” 与 “利用率（`U`）”，优先调度利用率最高的请求，缓解芯片冲突：$$ (TC(t) \leftarrow \sum_{k=0}^{P-1} \left( \min(RF^i(t), F_k(t)) - S_k(t) \right)) $$$$(U(t) \leftarrow \frac{TC(t)}{P \times (RF^i(t) - \min S_k(t))})$$ 其中`RF^i`为请求的虚拟完成时间（活跃芯片的最大虚拟结束时间），利用率越高，说明请求能更均匀地利用芯片，减少冲突。例如，某请求调度后能使 16 颗芯片的负载差异从 50% 降至 10%，利用率从 0.5 升至 0.9，优先被调度 。

###### （2）deficit 时间补偿

为避免请求因低利用率被长期重排序，为每个等待请求维护 “deficit 时间”，每次调度后累加未被选中请求的 deficit 时间，下次计算利用率时叠加该值，提升其被选中概率：$$ (U(t) \leftarrow \frac{TC(t) + D(t-1)}{P \times (RF^i(t) - \min S_k(t))}) $$当 deficit 时间超过总负载时，请求被强制调度，确保长尾延迟有界（99.9% 分位延迟控制在 1ms 以内）。例如，某请求因低利用率被延后 3 次，deficit 时间累加至总负载的 1.2 倍，下次调度时被强制选中，避免饥饿 。

##### 4. 方案效果验证

基于 128GB（16 颗芯片）与 256GB（32 颗芯片）SSD，在四类服务器 workload 上测试，DLBQ 表现如下：

- **吞吐量提升**：相比 NOOP、FlashFQ 等方案，128GB SSD 吞吐量提升 11%，256GB SSD 提升 15%，其中数据库 workload 提升最显著（28%） ；
- **延迟控制**：99.9% 分位尾延迟比 PIQ 低 40%，比 ParDispatcher 低 52%，且读写请求延迟比均衡（读延迟 55μs、写延迟 720μs，无明显阻塞） ；
- **扩展性**：在 256 线程高并发场景下，吞吐量比 FlashFQ 高 30%，芯片利用率达 85%，远高于传统方案的 60% 。

#### 四、总结

DLBQ 的核心创新在于打破了传统 SSD 调度 “单一维度优化” 的局限，通过：

1. **虚拟时间成本模型**：实时感知芯片负载，为调度决策提供精准依据；
2. **动态队列选择**：自适应分离读写请求，避免读性能被写操作挤压；
3. **均衡芯片利用率 + deficit 补偿**：缓解芯片冲突的同时，控制长尾延迟。

该方案尤其适用于服务器多应用共享场景，能在提升 SSD 吞吐量的同时保障 QoS，为 SSD 调度提供了 “性能 - 延迟” 双优的新范式。

### Self-Adapting Channel Allocation for Multiple Tenants Sharing SSD Devices
#### 一、研究背景与问题

##### 1. SSD 基础架构与多租户共享场景

- **SSD 硬件架构**：含控制器、多通道（连接闪存芯片）、DRAM 缓冲，核心并行单元为通道→芯片→裸片→平面→块→页，其中**裸片是命令执行基本单元**，**块是擦除基本单元**，**页是读写基本单元**。
- **多租户共享需求**：3D 堆叠技术提升 SSD 容量（如数 TB 级），单 SSD 可承载多租户（如数据库实例、应用存储），但需解决资源竞争问题。

##### 2. 传统通道分配的核心问题

|分配方式|具体做法|问题表现|典型场景影响|
|---|---|---|---|
|传统 SSD 均匀条带化|所有租户数据跨所有通道均匀分布|多租户 I/O 请求冲突严重，读请求（高优先级）阻塞写请求|写主导租户延迟升高，写比例 < 30% 时总延迟比最优策略高 47.8%|
|开放通道 SSD 盲目分区|为每个租户固定分配 1 个 / 多个通道|资源浪费（读主导租户占用多通道）、并行性不足|8 通道 2 租户按 1:7 分配，写比例 > 40% 时写延迟骤升 10.6 倍|

##### 3. 关键观察：通道分配需适配访问模式

- 读主导租户与写主导租户并发时，**读请求优先级高于写请求**（闪存芯片读耗时远低于写），写延迟受读请求干扰显著；
- 单一策略无法适配所有场景：如 8 通道 2 租户中，1:7 分配在写比例 <30% 时最优，2:6 在写比例 40% 时最优，5:3 在写比例> 60% 时最优，性能差异最高达 10.6 倍。

#### 二、SSDKeeper 核心设计

SSDKeeper 运行于**闪存转换层（FTL）**，通过 “采集 - 学习 - 分配 - 平衡” 闭环实现自适应通道管理，架构如下图所示（文档 Fig.4）：

##### 1. 模块 1：特征收集器（Feature Collector）

- **核心目标**：采集多租户访问模式特征，为策略决策提供输入；
- **采集内容**：
    - **租户读写特性**：通过时间阈值 T 内的 I/O 类型判断（0 = 读主导，1 = 写主导），用 workloadID 区分不同租户；
    - **混合负载强度**：T 时间内总 I/O 请求数（如 10/T 表示 T 时间内共 10 个 I/O）；
    - **租户相对强度**：单个租户 I/O 数占总 I/O 数的比例（如 0.5 表示某租户占比 50%）；
- **输出示例**（8 通道 4 租户）：`[10/T, 0:1:1:1, 0.5:0.2:0.1:0.2]`，分别对应总强度、读写特性、相对强度。

##### 2. 模块 2：策略学习器（Strategy Learner）

- **核心目标**：训练适配多租户访问模式的通道分配模型，输出最优策略；
- **两步训练流程**：
    1. **标签生成**：
        - 构建策略池：根据租户数量确定策略数量（如 8 通道 2 租户对应 8 种策略：Shared、7:1、6:2…1:7；8 通道 4 租户对应 42 种策略）；
        - 遍历测试：对每种混合负载，遍历策略池执行，记录最低延迟策略作为标签；
    2. **模型训练**：
        - 模型选择：采用人工神经网络（ANN），支持其他机器学习算法；
        - 网络结构：输入层 9 个特征（如 4 租户的读写特性 4 维 + 相对强度 4 维 + 总强度 1 维）、隐藏层 64 个神经元、输出层 42 个神经元（对应 42 种策略）；
        - 训练方式：离线训练（主机或 SSD 控制器空闲时），用 SGD 优化参数，更新公式为：
            
            $(w_{jk}^l := w_{jk}^l - \eta \frac{\partial C}{\partial w_{jk}^l}, \quad b_{jk}^l := b_{jk}^l - \eta \frac{\partial C}{\partial b_{jk}^l})$
            
            其中\(w_{jk}^l\)为第 l 层第 j 行第 k 列权重，\(b_{jk}^l\)为偏置，\(\eta\)为学习率，C 为损失函数。

##### 3. 模块 3：通道分配器（Channel Allocator）

- **核心目标**：基于训练好的模型，实时输出适配当前负载的通道分配策略；
- **工作流程**：
    - 当特征采集时间 t=T 时，输入采集到的特征至 ANN；
    - 通过前向传播计算，输出最优策略（如 8 通道 4 租户对应 5:1:1:1）；
    - t>T 时，将租户 I/O 请求定向到分配的通道，避免跨通道冲突；
- **开销分析**：
    - 存储开销：权重与偏置总存储约\(\sum_{i=1}^L (4N_{i-1}N_i + 4N_i)\)字节（L 为层数，\(N_i\)为第 i 层神经元数），8 通道 4 租户场景仅需 18KB；
    - 计算开销：前向传播浮点乘法次数为\(\sum_{i=0}^{L-1} N_iN_{i+1}\)，实时性无影响。

##### 4. 模块 4：通道交换方案（I-Swap）

- **核心问题**：固定通道分配导致通道老化不均（写主导租户通道擦除频繁，读主导租户通道几乎无磨损）；
- **方案设计**：
    1. 为每个通道维护擦除计数器，初始化为 0；
    2. 当最大计数器与最小计数器差值达到阈值 k 时，触发通道交换；
    3. 交换时机：选择通道空闲时间（基于租户 I/O 间隔，如 MSR trace 中间隔 3-49ms）迁移数据，避免影响租户 I/O；
    4. 交换后：重置最大计数器为 0，继续监控；
- **效果验证**：MSR Cambridge trace 中，rsrch_0（写主导，高擦除率）与 mds_1（读主导，低擦除率）共享时，I-Swap 使两者通道擦除次数差异缩小至阈值内，寿命延长 3.7 倍。

#### 三、实验评估

##### 1. 实验环境

|配置项|详情|
|---|---|
|SSD 模拟器|通用闪存模拟器|
|SSD 参数|8 通道，TLC 闪存；页大小 4KB，块含 256 页|
|租户与负载|2/4 租户，混合读写负载（写比例 10%-90%）；MSR Cambridge 真实 trace|
|对比方案|传统 SSD（均匀条带化）、固定通道分配（如 7:1、4:4）|
|性能指标|读写延迟、总延迟、通道擦除次数、SSD 寿命|

##### 2. 核心实验结果

- **性能提升**：SSDKeeper 相比传统 SSD，读写总延迟平均降低 12.6%，写比例 30% 时最优策略（1:7）比传统条带化延迟低 47.8%；
- **寿命延长**：I-Swap 方案使通道擦除次数均匀化，相比无交换方案，SSD 寿命最高延长 3.7 倍；
- **适配性**：不同写比例下，SSDKeeper 均能选择最优策略，如写比例 40% 时选 2:6，写比例 70% 时选 5:3，适配性显著优于固定策略。

#### 四、结论

1. SSDKeeper 通过**机器学习驱动的动态通道分配**，解决了多租户共享 SSD 的访问冲突与资源浪费问题，兼顾性能与资源利用率；
2. I-Swap 方案有效平衡通道磨损，填补了多租户场景下 SSD 寿命优化的空白；
3. 该机制运行于 FTL，无需修改 SSD 硬件 / 固件，兼容性强，可广泛应用于数据中心多租户存储场景。
### 论文《A_Latency-Aware_Garbage_Collection_Strategy》详细解析与多租户场景适配性分析

#### 一、论文核心背景与问题提出

##### 1. 底层技术前提

NAND 闪存的 “写前擦除” 特性，使得 SSD 需采用 “异地更新” 策略（即修改数据时不覆盖原页，而是写入新页并标记原页为无效），但无效页会占用空间，因此需定期执行**垃圾回收（GC）** —— 将某块内有效页迁移到新块，再擦除旧块释放空间。然而，GC 过程会占用 SSD 控制器资源与 IO 带宽，干扰正常用户 IO，引发**长尾延迟**（少数请求等待时间远超平均）。

同时，为提升 SSD 容量，厂商推出 “多比特 NAND 闪存”（如 TLC/QLC），其字线（Word-line）包含性能与可靠性差异显著的物理页：**LSB 页**（最低有效位页）延迟低、性能优，**MSB 页**（最高有效位页）延迟高、性能差，且芯片测试显示 MSB 页 I/O 延迟是 LSB 页的数倍。

##### 2. 现有方案的局限

- **未利用页类型差异**：传统 GC 优化策略（如贪心策略 GR-GC）仅关注 “优先擦除无效页多的块”，未考虑 GC 迁移有效页时的目标页类型，无法借助 LSB 页的低延迟优势缓解长尾延迟；
- **基础优化的缺陷**：虽有 “GC 时将有效页迁移到 LSB 页” 的思路，但无节制使用 LSB 页会导致空间浪费（如冷数据占用高价值 LSB 空间），无法平衡性能与空间利用率；
- **资源争抢严重**：传统策略未分离 GC 迁移数据与正常用户写入数据，GC 与用户 IO 争抢存储资源（如块、带宽），进一步加剧延迟问题。

#### 二、论文核心方案：延迟感知 GC 策略（LA-GC）

##### 1. 设计理念

LA-GC 的核心是 **“动态适配负载状态，协同优化页类型分配与数据分离”** —— 通过感知 SSD 当前负载压力（队列深度），动态调整 GC 迁移有效页的目标页类型，同时分离 GC 数据与正常用户数据，在提升性能的同时避免空间浪费。

##### 2. 关键模块与执行逻辑

###### （1）核心模块设计

基于传统 SSD 主控架构，新增两类模块：

- **负载压力动态感知模块**：实时监控设备队列深度（即 SSD 当前等待处理的用户 IO 请求数量），判断负载状态（队列深则负载高，队列浅则负载低）；
- **页类型感知页分配模块**：根据负载状态动态选择 GC 迁移页的类型，同时记录各平面内 LSB/MSB 页的剩余数量，避免空间浪费。

###### （2）具体执行流程

1. **页类型判定**：每处理 100 个用户请求，重新计算平均队列长度，更新负载状态；
2. **目标页类型选择**：
    - 负载高（队列深，用户 IO 密集）：优先将有效页迁移到 LSB 页，利用其低延迟特性减少 GC 对用户 IO 的干扰，缓解长尾延迟；
    - 负载低（队列浅，用户 IO 稀疏）：将有效页迁移到 MSB 页，保留 LSB 页空间供后续高负载时使用，避免空间浪费；
3. **数据分离**：将 GC 迁移数据与正常用户写入数据分开存储，避免两者争抢块与带宽资源。

##### 3. 开销分析

LA-GC 虽引入额外开销，但整体可忽略，不影响性能收益：

- **时间开销**：时间复杂度为 O (n)（n 为平面尾部块数量），仅需查询块的活跃页类型，非高耗时操作；耗时来源包括 “统计队列平均长度” 与 “搜索特定页类型”，均在可接受范围；
- **空间开销**：需记录每个平面内 LSB/MSB 页的剩余数量，采用 int 变量存储，每平面仅占用 12 字节，相对 SSD 容量可完全忽略。

#### 三、实验设计与结果验证

##### 1. 实验配置

###### （1）硬件与模拟器

基于**SSDsim 模拟器**（领域内广泛使用，高仿真精度且参数可配置）构建实验环境，模拟 NVMe SSD 系统，核心参数如下：

|参数|数值|
|---|---|
|总容量|1TB|
|预留空闲块|10%|
|通道数|8|
|每通道芯片数|4|
|每芯片 die 数|4|
|每 die 平面数|4|
|每平面块数|1024|
|每块页数|512|
|页大小|4KB|
|每页子页数|8|

###### （2）工作负载与对比策略

- **工作负载**：选用 MSR Cambridge traces 中的 8 类典型负载（如 HM_0、PRN_0、PROJ_1 等），覆盖不同读写比例（如 HM_0 写比 75%、PRXY_0 写比 97%）、更新比例（如 HM_0 更新比 90%、PROJ_1 更新比 22%）与请求间隔，实验前填充数据以确保 GC 触发；
- **对比策略**：
    1. GR-GC（传统贪心策略）：优先擦除无效页最多的块，迁移有效页到同平面活跃块；
    2. DS-GC（数据分离策略）：仅分离 GC 迁移数据与正常写数据，无页类型优化；
    3. TP-GC（特定页类型策略）：仅固定分配 GC 目标页类型，无负载感知；
    4. LA-GC（论文方案）：负载感知 + 页类型优化 + 数据分离。

##### 2. 核心实验结果

###### （1）百分位延迟（长尾延迟优化）

聚焦 94% 分位后的数据（突出长尾延迟），同百分位下 LA-GC 的延迟显著低于 GR-GC，DS-GC 与 TP-GC 介于两者之间。例如，在高写比负载 HM_0 中，99% 分位延迟 LA-GC 比 GR-GC 降低约 23%，原因是 LA-GC 动态分配页类型并分离数据，减少了 GC 对用户 IO 的排队干扰。

###### （2）平均响应时间

与 GR-GC 相比，LA-GC 使**读请求平均响应时间降低 10.9%**，**写请求平均响应时间降低 15.8%**。写请求优化幅度更大，是因为传统策略中 GC 与写请求争抢 “块写入资源” 更激烈，而 LA-GC 的数据分离与 LSB 页分配有效缓解了这一问题。

###### （3）GC 平均迁移页数

LA-GC 相比 GR-GC 减少 17% 的 GC 迁移页数，与 DS-GC 效果一致。这是因为两者均通过 “数据分离” 避免了不必要的有效页迁移，缩短了 GC 任务耗时，进一步降低对用户 IO 的影响。

#### 四、多租户场景适配性分析

多租户场景的核心需求是 **“隔离性”（租户间性能不干扰）、“公平性”（各租户资源分配均衡）、“低延迟”（租户请求响应稳定）**，结合 LA-GC 的设计特性，其在多租户场景下的适配性可从 “可行性”“优势”“需优化点” 三方面展开：

##### 1. 可行性：LA-GC 的核心机制适配多租户基础需求

- **负载感知能力匹配租户动态负载**：多租户场景下，不同租户的 IO 负载波动差异大（如租户 A 高峰期写请求密集，租户 B 长期处于低负载），LA-GC 的 “队列深度监控” 可实时感知单个租户或租户集群的负载状态，为不同租户动态分配页类型（高负载租户用 LSB 页，低负载租户用 MSB 页），避免单一租户的 GC 影响其他租户；
- **数据分离机制支持租户隔离**：LA-GC 的 “GC 数据与正常数据分离” 可扩展为 “租户间数据分离”—— 为不同租户划分独立的存储区域（如独立平面或块组），并在各区域内执行 LA-GC 策略，避免租户 A 的 GC 迁移数据占用租户 B 的 LSB 页资源，保障隔离性。

##### 2. 优势：LA-GC 可解决多租户场景的关键痛点

- **缓解 “租户间 GC 干扰”**：传统 GC 策略中，某租户触发 GC 时会占用全局 IO 带宽，导致其他租户请求延迟飙升；LA-GC 通过 “动态页类型分配” 缩短 GC 耗时，同时 “数据分离” 限制 GC 资源占用范围，减少跨租户干扰；
- **提升资源利用率与公平性**：多租户场景下，LSB 页是稀缺资源，传统策略易被高负载租户独占；LA-GC 根据负载动态分配 LSB 页，低负载租户释放的 LSB 页可分配给高负载租户，实现 “按需分配”，提升资源利用率的同时保障各租户的性能公平；
- **稳定租户请求延迟**：多租户场景对 “延迟稳定性” 要求高（如金融租户、实时数据租户），LA-GC 优化的长尾延迟特性，可减少因 GC 导致的租户请求 “突增延迟”，保障服务质量（SLA）达标。

##### 3. 需优化点：适配多租户场景的扩展设计

- **租户级负载感知与资源配额**：现有 LA-GC 仅感知全局队列深度，需扩展为 “租户级负载监控”—— 为每个租户设置独立的队列深度阈值与 LSB 页配额（如为高优先级租户分配更多 LSB 页），避免低优先级租户占用过多高优资源；
- **GC 优先级调度**：多租户场景下，高优先级租户的 IO 请求应优先于低优先级租户的 GC 任务；需在 LA-GC 中增加 “租户优先级判定模块”，当高优租户有请求时，暂停低优租户的 GC 任务，优先处理高优请求；
- **租户级性能监控与计费适配**：多租户场景需按租户实际资源消耗（如 LSB 页使用量、GC 次数）计费，需扩展 LA-GC 的 “页类型记录模块”，统计各租户的资源占用数据，为计费系统提供支撑。

#### 五、总结

论文针对多比特 NAND 闪存 SSD 的 GC 长尾延迟问题，提出 “延迟感知 GC 策略（LA-GC）”，通过 “负载感知动态页类型分配” 与 “数据分离”，实现了 “长尾延迟缓解、平均响应时间降低、GC 效率提升” 的目标，且开销可忽略。在多租户场景下，LA-GC 的核心机制（负载感知、数据分离）具备基础适配性，能解决租户间 GC 干扰、资源分配公平性等痛点，通过 “租户级负载监控、GC 优先级调度” 等扩展设计后，可成为多租户 SSD 存储系统的高效 GC 方案。