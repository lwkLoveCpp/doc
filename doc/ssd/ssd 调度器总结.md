### **《Sprinkler: Maximizing Resource Utilization in Many-Chip Solid State Disks》**（2014，HPCA）
#### 一、先搞懂核心目标：Sprinkler 要解决什么 “麻烦”？​

先用 “快递仓库分拣” 类比多芯片 SSD 的痛点，帮你理解设计的出发点：​

- 场景类比：某快递仓库有 8 个分拣通道（对应 SSD 的 8 个通道），每个通道有 8 个分拣员（对应 8 个芯片），需处理大量快递（对应主机 I/O 请求）。​

- 传统问题（VAS/PAS 的缺陷）：​

1. 并行性依赖：快递按到达顺序分配，导致通道 0 的 8 个分拣员忙到加班，通道 1 的分拣员却闲坐着（芯片间空闲率高）；​

2. 事务效率低：每个快递单独分拣（单请求→单事务），分拣员频繁切换操作，效率低下（FLP 低）。​

- Sprinkler 的目标：让所有分拣员（芯片）尽量忙起来，同时让多个快递打包一起分拣（合并事务），提升整体效率。​

#### 二、用 “仓库分拣” 类比拆解两大核心组件​

组件 1：RIOS（资源驱动 I/O 调度）—— 让分拣员 “均匀忙起来”​

RIOS 的核心是 “不按快递到达顺序，按分拣员空闲状态分配”，对应 SSD 中 “不按 I/O 顺序，按芯片 /die 空闲状态调度”，具体分 3 步：​

1. 第一步：给每个快递贴 “资源标签”（PhyResourceTag）​

- 类比：每个快递到达后，先贴标签注明 “需通道 2 - 分拣员 3 处理”（对应 SSD 中，I/O 请求经 FTL 转换后，生成含 “通道号 - 芯片号 - die 号” 的 PhyResourceTag）；​

- 关键数据结构：仓库有张 “分拣员状态表”（对应 phyLayout 表），记录每个通道 - 分拣员是否空闲（如通道 2 - 分拣员 3 当前空闲），支持 1 秒内查完所有状态（O (1) 查询）。​

2. 第二步：跨通道 “轮询找空闲分拣员”（跨通道资源遍历）​

- 传统方式（PAS）：按通道顺序分配，先把所有快递给通道 0，通道 0 满了再给通道 1，导致通道 0 拥堵；​

- RIOS 创新方式：“跨通道同偏移轮询”，比如先找所有通道的 “分拣员 0”（通道 0 - 分拣员 0、通道 1 - 分拣员 0...），再找 “分拣员 1”，具体步骤（对应伪代码逻辑）：​

1. 从 “偏移 0” 开始（即先找所有通道的分拣员 0）；​

2. 逐个通道检查：通道 0 - 分拣员 0 空闲？是就分配快递；通道 1 - 分拣员 0 空闲？是就分配；​

3. 一轮结束后，偏移 + 1（下次找所有通道的分拣员 1），循环直到所有快递分配完；​

- 类比效果：原本通道 0 - 分拣员 0-7 忙到爆，现在通道 0 - 分拣员 0、通道 1 - 分拣员 0、通道 2 - 分拣员 0 同时处理快递，8 个通道的分拣员均匀忙碌，芯片间空闲率降低 46.1%（实验数据）。​

3. 第三步：避免 “通道内拥堵”（冲突规避）​

- 问题：同一通道的 8 个分拣员共享 1 个传送带（对应通道总线），不能同时操作；​

- RIOS 解决：“通道内串行，通道间并行”—— 通道 0 的分拣员 0 处理快递时，通道 0 的其他分拣员等待，但通道 1 - 分拣员 0 可同时处理，既避免传送带冲突，又保证多通道并行。​

组件 2：FARO（FLP 感知请求过提交）—— 让快递 “打包一起分拣”​

FARO 的核心是 “多快递打包成 1 批处理”，对应 SSD 中 “多请求合并成 1 个高并行事务”，解决 “单请求处理效率低” 的问题，具体分 3 步：​

1. 第一步：“多收快递备用”（请求过提交）​

- 传统方式：分拣员处理完 1 个快递，再收下 1 个，中间有空等时间；​

- FARO 创新方式：提前收 2 倍于当前处理能力的快递（过提交比例 1.5-2.0 倍），比如当前能处理 10 个快递，先收 20 个存在 “候选区”，避免分拣员空等。​

2. 第二步：“挑合适的快递打包”（优先级评分）​

- 问题：不是所有快递都能打包 —— 比如 A 快递需通道 2 处理，B 快递需通道 3 处理，打包一起会跨通道，效率反而低；​

- FARO 解决：按 “3 个维度” 给候选区的快递打分，优先选高分快递打包：​

1. 重叠深度（α=0.6）：快递是否集中在同一通道的 “多分拣员 + 多操作台”（对应 SSD 中请求是否覆盖多 die/plane），比如某批快递都需通道 2 的 “分拣员 0-1（对应 2 个 die）+ 每个分拣员的 4 个操作台（对应 4 个 plane）”，重叠深度 = 2×4=8（满分），优先选；​

2. 连通性（β=0.3）：快递是否来自同一客户（对应请求是否同属一个主机 I/O），比如某客户一次发 10 个快递，连通性高，优先打包；​

3. 冲突风险（γ=0.1）：快递需用的分拣员是否快忙完（对应资源是否即将空闲），风险低的优先选；​

- 评分公式：Score=0.6× 重叠深度 + 0.3× 连通性 - 0.1× 冲突风险，选前 8 个高分快递打包。​

3. 第三步：“打包后高效分拣”（事务合并）​

- 类比操作：将 8 个高分快递打包成 1 批，分配给通道 2 的 “2 个分拣员（die0-1）+ 每个分拣员的 4 个操作台（plane0-3）”，具体分工：​

- 分拣员 0（die0）用 4 个操作台同时处理 4 个快递（plane sharing）；​

- 分拣员 1（die1）用 4 个操作台同时处理另外 4 个快递（die interleaving）；​

- SSD 对应效果：原本 8 个请求需 8 个事务处理，现在 1 个事务搞定，事务数减少 50.2%，FLP（闪存并行性）提升 80.2%，处理速度翻倍。​

#### 三、两大组件如何配合？用 “快递处理全流程” 看协同​

以 “主机发 10 个 I/O 请求” 为例，看 RIOS 和 FARO 如何配合工作（对应原文 mermaid 流程图）：​

1. 快递到达（I/O 请求）：10 个快递到仓库，先贴 “资源标签”（FTL 生成 PhyResourceTag）；​

2. 录入系统（存储）：标签存入 “分拣员状态表”（RIOS 的 phyLayout 表），快递放入 “候选区”（FARO 的候选池）；​

3. 找空闲分拣员（RIOS 调度）：RIOS 按 “跨通道轮询” 找空闲分拣员，标记出通道 2 - 分拣员 0-1、通道 3 - 分拣员 0 空闲；​

4. 打包快递（FARO 合并）：FARO 给候选区的 10 个快递打分，选 8 个高分的打包成 1 批，刚好匹配通道 2 的 2 个分拣员 + 8 个操作台；​

5. 分配处理（执行）：RIOS 把打包好的快递分配给通道 2 的分拣员，同时把剩下 2 个快递分配给通道 3 的分拣员；​

6. 更新状态（反馈）：处理完后，更新 “分拣员状态表” 和 “候选区”，准备处理下一批。​

关键协同点：RIOS 告诉 FARO “哪些分拣员空闲”，FARO 根据这个信息打包合适的快递；FARO 打包好后，RIOS 再把快递分配给对应的分拣员，二者缺一不可。​

#### 四、特殊情况怎么处理？比如 “分拣员请假（GC 迁移）”​

- 场景：通道 2 - 分拣员 0 要请假（对应 SSD 中该芯片需 GC 迁移数据），手里的快递还没处理完；​

- Sprinkler 解决：​

1. 实时通知（重寻址回调）：仓库管理员（FTL）立刻告诉 RIOS 和 FARO “分拣员 0 请假”，更新 “分拣员状态表”；​

2. 转移快递（数据迁移）：把分拣员 0 手里的快递转给通道 2 - 分拣员 2（新物理地址），重新贴标签；​

3. 调整优先级（保障性能）：迁移期间，请假分拣员的工作优先级降低，优先处理其他快递，避免影响整体效率。​

#### 五、一句话总结设计核心​

Sprinkler 的设计本质是 “用 RIOS 解决‘谁来干’的问题（让资源均匀忙），用 FARO 解决‘怎么干’的问题（让干活更高效） ”，二者配合突破传统调度的瓶颈，最终实现 “多芯片 SSD 性能提升 1.8-2.2 倍，资源利用率提升 68.8%” 的效果。​


### ==《Workload-Aware Budget Compensation Scheduling for NVMe Solid State Drives》核心内容解析==

#### 一、论文提出的核心问题

随着 NVMe SSD 在数据中心的广泛应用，基于 SR-IOV（单根 I/O 虚拟化）的 SSD 虚拟化技术成为云存储的关键组件 —— 通过将物理 SSD 虚拟化为多个虚拟设备（VF），实现多虚拟机（VM）直接共享 SSD，减少虚拟机监控器（VMM）的软件开销。但作者发现，现有 SR-IOV SSD 的调度方案在多租户场景下存在三大关键问题，导致**性能隔离失效**与**资源分配不公**：

##### 1. 问题 1：SSD 内部操作（尤其是 GC）导致跨 VM 性能干扰

SSD 的垃圾回收（GC）是影响性能的核心内部操作，但其开销具有 “跨 VM 传递” 特性：

- **GC 触发与影响的非对称性**：产生大量随机小写请求的 VM（如数据库 VM）会频繁触发 GC（因随机写生成大量无效页），但 GC 的延迟开销会扩散到其他 VM—— 例如，VM A 的随机写触发 GC 后，VM B 的读请求因通道被 GC 占用，延迟从 50μs 升至 750μs，性能下降 14 倍 ；
- **开销归属无法量化**：传统方案将 GC 开销均匀分摊给所有 VM，未区分 “GC 制造者” 与 “GC 受害者”。例如，VM C（随机写，GC 贡献度 90%）与 VM D（顺序写，GC 贡献度 10%）共享 SSD 时，两者均承担 50% 的 GC 延迟，导致 VM D 的性能被不合理挤压 ；
- **延迟传递的滞后性**：GC 通常在无效页积累到阈值后触发，某 VM 的写操作可能在数分钟后才引发 GC，此时该 VM 可能已释放资源，而其他新启动的 VM 会承担其遗留的 GC 开销，进一步加剧不公 。

##### 2. 问题 2：读写请求成本估算未适配 VM workload 特性

现有调度方案（如 BCQ、FlashFQ）虽尝试区分 SSD 的读写成本，但存在两大缺陷：

- **成本模型静态化**：采用离线校准或固定系数估算读写成本（如假设写成本是读成本的 10 倍），未考虑不同 VM workload 的动态差异。例如，VM E 的顺序写请求实际 GC 开销仅为读成本的 5 倍，而 VM F 的随机写请求 GC 开销是读成本的 20 倍，统一系数会导致 VM E 的预算被过度扣除，VM F 的预算不足 ；
- **未区分写请求的类型差异**：同样是写请求，随机写与顺序写的 GC 贡献度差异显著（随机写的写放大（WAF）是顺序写的 3-5 倍），但现有方案将所有写请求视为同等成本，导致 “高 GC 贡献 VM 未被惩罚，低 GC 贡献 VM 被误伤” 。

##### 3. 问题 3：SR-IOV SSD 的调度缺乏 VM 级性能隔离机制

SR-IOV SSD 为每个 VM 分配独立的请求队列，但现有调度仅基于 “请求到达时间” 或 “固定权重” 排序，未实现 VM 级的性能隔离：

- **队列级调度的局限性**：传统队列调度（如 Round-Robin）仅保证请求数量的公平，未考虑请求的实际资源消耗。例如，VM G 的 1 个随机写请求（资源消耗 20 单位）与 VM H 的 1 个读请求（资源消耗 1 单位）按 1:1 调度，实际资源占用比为 20:1，VM H 的性能被严重压制 ；
- **缺乏 VM 级预算补偿**：当某 VM 因其他 VM 的 GC 开销导致性能下降时，现有方案无动态预算补偿机制。例如，VM I 的读延迟因 GC 升高 50%，但仍按原预算分配资源，无法满足其服务等级协议（SLA） 。

##### 4. 问题 4：多流 SSD 架构下的资源分配未关联 VM workload 特征

部分方案（如多流 SSD）虽为每个 VM 分配专属的擦除块组（EBG），但未利用 EBG 的 workload 特征优化调度：

- **EBG 的 WAF 价值未被挖掘**：每个 VM 的 EBG 可通过写放大（WAF）量化其 GC 贡献度（WAF 越高，GC 贡献越大），但现有方案未将 WAF 与资源分配绑定，导致 “高 WAF VM（GC 制造者）与低 WAF VM（GC 受害者）获得同等资源” ；
- **块级隔离未转化为调度优势**：多流 SSD 的 EBG 隔离本可实现 VM 级开销统计，但现有调度仍将 SSD 视为黑盒，未利用 EBG 的 workload 数据（如无效页比例、擦写次数）优化预算分配 。

#### 二、发现问题的方式

作者通过 “架构缺陷分析 + 实验对比验证 + workload 特性拆解” 的三层方法，系统性发现并量化问题，具体如下：

##### 1. 架构缺陷的对比实验

构建 “SR-IOV SSD 多 VM 共享” 实验环境，对比传统方案与理想隔离状态的性能差异：

- **GC 跨 VM 干扰验证**：在 16 通道 NVMe SSD 上，部署 2 个 VM（VM1：随机写，VM2：顺序读），通过跟踪 GC 触发时间与 VM 延迟发现：VM1 的随机写每 10 分钟触发 1 次 GC，每次 GC 导致 VM2 的读延迟升高 14 倍，且 VM2 的性能波动与 VM1 的写操作频率高度相关（相关系数 0.89），证明 GC 开销的跨 VM 传递 ；
- **成本模型合理性验证**：测试 4 种典型 workload（顺序读、顺序写、随机读、随机写）的实际资源消耗，发现：随机写的单位资源消耗是顺序读的 20 倍，而传统方案的固定系数（10 倍）会导致随机写 VM 的预算透支（实际消耗超预算 100%），顺序读 VM 的预算闲置（仅消耗 50%） 。

##### 2. 现有方案的缺陷量化

基于 SSDsim 模拟器，对比主流调度方案（FIFO、BCQ、FlashFQ）在多 VM 场景下的性能：

- **性能隔离失效**：在 4-VM 场景（VM1：顺序读，VM2：顺序写，VM3：随机读，VM4：随机写）中，BCQ 调度下 VM1 的读延迟因 VM4 的随机写升高 52%，而理想隔离状态下仅升高 5%，证明现有方案无法隔离跨 VM 干扰 ；
- **预算分配不公**：FlashFQ 调度按 “请求数 1:1” 分配资源，VM4（随机写）与 VM1（顺序读）的实际资源占用比为 20:1，VM1 的吞吐量仅为理想状态的 5%，验证成本模型未适配 workload 特性的缺陷 ；
- **长尾延迟失控**：FIFO 调度下，低 GC 贡献 VM 的 99.9% 分位延迟达 25ms，是高 GC 贡献 VM 的 3 倍，违反 SLA 对尾延迟的约束（通常要求 < 1ms） 。

##### 3. workload 特性拆解

分析多 VM 场景下的 workload 特征，明确问题根源：

  

|VM 类型|读写比|请求模式|WAF 值|GC 贡献度|单位资源消耗（相对值）|
|---|---|---|---|---|---|
|Web 服务器|9:1|随机读|1.2|5%|1|
|数据库|1:9|随机写|4.5|85%|20|
|日志存储|1:9|顺序写|1.8|10%|5|
|文件服务器|3:7|混合读写|2.3|30%|8|

  

通过拆解发现，不同 VM 的 GC 贡献度与资源消耗差异达 40 倍，而现有方案的 “统一调度” 与 “均匀分摊” 完全无法适配这种差异，必然导致性能隔离失效 。

#### 三、解决方案：Workload-Aware Budget Compensation（WA-BC）调度

针对上述问题，作者提出 **“workload 感知的预算补偿调度”**，核心是通过 “VM 级 GC 贡献量化 + 动态预算补偿” 实现多 VM 的性能隔离，具体设计如下：

##### 1. 整体架构

WA-BC 作为 SR-IOV SSD 的设备级调度器，位于 VF 请求队列与闪存通道队列之间，包含三大核心模块：

- **多流 EBG 管理模块**：为每个 VM 分配专属的擦除块组（EBG），确保 VM 的数据不跨 EBG 混合，实现 VM 级的开销统计；
- **回归式成本建模模块**：实时估算读写请求的动态成本，适配不同 VM 的 workload 特性；
- **预算补偿模块**：基于 VM 的 GC 贡献度（通过 WAF 量化），动态调整各 VM 的时间预算，实现 “谁制造开销谁承担” 的公平性 。

架构上，WA-BC 与 SR-IOV SSD 的集成如图 1 所示：每个 VM 通过专属 VF 发送请求，请求先进入 VM 对应的请求队列，WA-BC 调度器根据预算与 GC 贡献度选择队列，再将请求分发到对应的闪存通道队列（按 FTL 的芯片分配算法），实现并行处理 。

##### 2. 核心技术 1：多流 EBG 管理 ——VM 级开销隔离与统计

基于多流 SSD 架构，为每个 VM 分配独立的擦除块组（EBG），实现 “数据隔离 + 开销可追溯”：

- **EBG 专属分配**：每个 VM 的所有写请求仅写入其专属 EBG，无效页与有效页均在 EBG 内管理，避免不同 VM 的数据混合导致的开销归属混乱。例如，VM1 的随机写生成的无效页仅存在于其 EBG1 中，GC 时仅需处理 EBG1，不会影响其他 EBG ；
- **EBG 状态监控**：实时跟踪每个 EBG 的关键指标：
    - 写放大（WAF）：通过公式$(\alpha_i = u_i / v_i)$计算（$(u_i)$为 EBG 的总页数，$(v_i)$为有效页数），WAF 越高，VM 的 GC 贡献度越大；
    - 无效页比例：记录 EBG 内无效页占比，用于预测 GC 触发时机；
    - 擦写次数：用于辅助磨损均衡，避免 EBG 因过度使用提前老化；
- **动态 EBG 调整**：当 VM 的 workload 变化（如从顺序写转为随机写）时，自动为其扩容 EBG（从空闲块池分配新块），确保 EBG 有足够空间容纳无效页，减少频繁 GC 。


该模块的核心价值是将 SSD 从 “黑盒” 转为 “半透明盒”，为后续 GC 贡献度量化与预算补偿提供数据基础 。

##### 3. 核心技术 2：回归式读写成本建模 —— 动态适配 VM workload

WA-BC 采用在线线性回归模型，实时估算不同 VM 的读写请求成本，解决传统静态模型的缺陷：

- **成本模型定义**：将每个 profiling 周期（如 100ms）的总执行时间\(T_i\)分解为读请求成本与写请求成本的线性组合：$(T_i = C_R × N_R(i) + C_W × N_W(i))$ 其中，$(N_R(i))$、$(N_W(i))$分别为周期内读、写请求数（均为页级请求，4KB），$(C_R)$、$(C_W)$为待估算的读、写单位成本（单位：μs / 页）；
- **在线回归估算**：每积累 K 个 profiling 周期（默认 K=20），通过最小二乘法求解$(C_R)$与$(C_W)$，确保成本模型实时反映 SSD 的内部状态（如 GC、磨损均衡的动态变化）。例如，当 GC 频繁时，$(C_W)$会从 100μs / 页升至 200μs / 页，模型可自动捕捉该变化 ；
- **VM 级成本校准**：基于多流 EBG 的监控数据，为每个 VM 的读写请求附加 “workload 系数”—— 例如，VM 的随机写请求成本为$C_W × \beta$ , $β$为该 VM 的 WAF 与全局 WAF 的比值），顺序写请求成本为$(C_W × 0.5)$，进一步提升成本估算的精准度 。

  

该模型的开销极小：在 ARM Cortex-R4 处理器（400MHz）上，回归计算仅需 1000 个 CPU 周期（3μs）与 320 字节代码空间，完全可忽略 。

##### 4. 核心技术 3：预算补偿机制 —— 基于 GC 贡献度的公平分配

WA-BC 的核心创新是通过 “GC 贡献度量化→成本补偿→预算调整” 的闭环，实现跨 VM 的性能隔离：
###### （1）GC 贡献度量化

基于多流 EBG 的 WAF 值，计算每个 VM 对 GC 开销的贡献度：

- **全局 GC 成本分解**：假设全局写成本\(C_W\)包含基础写成本与 GC 附加成本，通过全局 $WAF(\alpha = \sum u_i / \sum v_i)$分离两者：$(C_W' = C_W / \alpha)$ 其中，$(C_W')$为无 GC 时的基础写成本，$(C_W - C_W')$为全局 GC 附加成本；
- **VM 级 GC 贡献度计算**：每个 VM 的 GC 附加成本与其 EBG 的 $WAF（\alpha_i）$成正比，因此 VM i 的实际写成本为：$(C_{W_i} = C_W × \frac{\alpha_i}{\alpha})$

###### （2）动态预算分配与补偿

基于量化的 GC 贡献度，WA-BC 为每个 VM 分配 “基础预算 + 补偿预算”：

- **基础预算分配**：按 VM 的权重（由 SLA 确定）分配初始时间预算，例如权重 1:1 的两个 VM，初始预算均为 10ms / 周期；
- **预算扣除规则**：处理 VM 的请求时，按其实际成本扣除预算 —— 读请求扣除\(C_R\)，写请求扣除$(C_{W_i})$。例如，VM1 的 1 个写请求扣除 320μs，VM2 的 1 个写请求扣除 120μs，确保预算消耗与 GC 贡献度匹配；
- **补偿预算机制**：若某 VM 的预算因其他 VM 的 GC 干扰提前耗尽（如 VM2 的预算因 VM1 的 GC 被占用而快速消耗），WA-BC 从 “高 GC 贡献 VM 的闲置预算” 中划拨补偿预算给该 VM，例如从 VM1 的剩余预算中划拨 2ms 给 VM2，确保其能继续处理请求 ；
- **预算 replenishment**：当所有 VM 的预算均耗尽或周期结束时，重新计算$(C_R)$、$(C_W)$与$(C_{W_i})$，并按新的成本模型分配下一轮预算，实现动态适配 。

##### 5. 方案效果验证

基于 SSDsim 模拟器（带 SR-IOV 与多流 EBG 扩展），在 4 类典型场景下验证 WA-BC 的有效性：

###### （1）场景 1：2 个 VM（顺序写 vs 随机写）

- **问题**：传统 BCQ 调度下，顺序写 VM 的延迟因随机写 VM 的 GC 升高 52%；
- **WA-BC 效果**：顺序写 VM 的延迟波动从 52% 降至 8%，随机写 VM 的预算消耗是顺序写 VM 的 2.7 倍，实现性能隔离 。

###### （2）场景 2：4 个 VM（不同 WAF 值）

- **问题**：BCQ 调度下，高 WAF VM（WAF=4.2）与低 WAF VM（WAF=2.99）的延迟差异仅 10%，未体现 GC 贡献度差异；
- **WA-BC 效果**：高 WAF VM 的延迟是低 WAF VM 的 3.5 倍，预算消耗比例与 WAF 比例完全匹配（4.2:2.99≈1.4:1），实现公平性 。

###### （3）场景 3：4 个 VM（混合读写 workload）

- **问题**：FIFO 调度下，读密集 VM 的延迟因写密集 VM 的 GC 升高 14 倍；
- **WA-BC 效果**：读密集 VM 的延迟波动 < 10%，写密集 VM 的预算消耗按 GC 贡献度差异化分配，读、写 VM 的性能隔离度达 90% 。

###### （4）场景 4：真实 workload（WebProxy、FileCopy、FileServer、Varmail）

- **问题**：BCQ 调度下，低 WAF 的 WebProxy VM（WAF=4.66）因高 WAF 的 FileServer VM（WAF=14.37）的 GC 干扰，吞吐量下降 40%；
- **WA-BC 效果**：WebProxy VM 的吞吐量恢复至理想状态的 95%，FileServer VM 的预算消耗是 WebProxy VM 的 3.1 倍，完全匹配两者的 GC 贡献度差异 。

#### 四、总结

WA-BC 方案的核心价值在于：首次将 SR-IOV SSD 的 “虚拟化调度” 与 “SSD 内部操作开销” 深度绑定，通过多流 EBG 实现 VM 级开销追溯，通过回归模型动态估算读写成本，通过

**《Architecting Flash-Based Solid-State Drive for High-Performance I/O Virtualization》**（2014，CAL）
# 《Preemptible I/O Scheduling of Garbage Collection for Solid State Drives》论文总结

## 一、论文提出的核心问题

论文聚焦 SSD（固态硬盘）因**垃圾回收（GC）与 I/O 请求串行执行导致的性能缺陷**，通过商用 SSD（COTS SSD）测试与理论分析，明确三大核心问题：

### 1. 性能下降：GC 占用带宽导致 I/O 阻塞

SSD 采用 “Out-of-place 更新” 机制，需通过 GC 回收无效页（流程：复制 victim 块有效页→擦除 victim 块），单次 GC 的块擦除耗时 1-2ms（约为写操作的 5-10 倍）。传统非可抢占式 GC（NPGC）一旦启动，会独占 SSD 内部带宽（通道、芯片、die/plane），I/O 请求需在队列中等待 GC 完成，导致请求延迟显著增加。实验显示，在写主导负载下，NPGC 的 I/O 响应时间是无 GC 场景的 3-4 倍（🔶6-10、🔶6-24）。

### 2. 性能波动：写主导负载下稳定性差

GC 触发依赖空闲块阈值，不同负载的 GC 触发频率差异大，导致 SSD 性能随负载特性剧烈波动。通过两款商用 SSD（Super Talent MLC、Intel SLC）测试发现：

- 80% 写负载的带宽波动频率是 40% 写负载的 2 倍以上；
- 写主导负载的变异系数（CV，衡量波动程度）显著更高，如 SSD (A)（MLC）80% 写负载 CV=0.03599，40% 写负载 CV 仅 0.03249（🔶6-21、🔶6-32）。这种波动对企业级应用（如 OLTP、HPC checkpointing）的服务质量（QoS）造成严重影响。

### 3. 资源浪费：SSD 并行能力未被利用

传统 NPGC 未结合 SSD 的硬件特性（多通道、多 die/plane、页缓冲）优化：

- 即使 SSD 存在空闲通道 / 芯片，若某一通道执行 GC，其他通道的空闲资源也无法被排队 I/O 利用；
- 页缓冲的高速访问能力被闲置，GC 与 I/O 请求无法复用缓冲数据，导致闪存访问次数冗余，通道利用率仅 37%-42%（🔶6-43、🔶6-163）。

## 二、论文的解决方案：两种可抢占式 GC 方案

论文针对上述问题，提出 “半可抢占 GC（semi-PGC）” 与 “完全可抢占 GC（F-PGC）” 两种方案，核心是通过 “低开销抢占 + 硬件特性复用” 实现 GC 与 I/O 的并行执行，同时保障数据一致性与空闲块充足性。

### 1. 基础方案：半可抢占 GC（semi-PGC）—— 无硬件扩展的低成本并行

#### （1）核心逻辑：识别 GC 流程的低开销抢占点

GC 的核心操作是 “页移动”（单页的 “读→传输→写→元数据更新”），论文在 “页移动” 中识别两个可插入 I/O 的抢占点，实现 GC 与 I/O 的并行（🔶6-33、🔶6-35）：

|抢占点|位置|并行机制|适用场景|
|---|---|---|---|
|A 点|页移动内（页写入前）|页缓冲被 GC 数据占用，仅支持 “同类型请求流水线”（如写 - 写并行）—— 需闪存支持命令流水线，新请求数据写入其他空闲页缓冲，与 GC 操作重叠执行|写密集场景（如 Cello 负载）|
|B 点|页移动间（前一页移动完成，后一页未开始）|页缓冲空闲，支持任意 I/O 请求（读 / 写）插入 ——GC 暂停，I/O 直接使用空闲缓冲，完成后恢复 GC|混合读写、随机请求场景（通用）|

#### （2）优化策略：减少资源竞争与延迟

- **请求合并**：若 I/O 请求与 GC 访问的页重叠（如 GC 读页 x 到缓冲，同时有读页 x 的 I/O），I/O 直接复用页缓冲数据，无需重新读取闪存，减少 1 次闪存访问（🔶6-50、🔶6-53）；
- **请求流水线**：同类型 I/O（如连续读 / 写）通过页缓冲实现 “操作重叠”—— 前一 I/O 的闪存编程与后一 I/O 的缓冲写入并行，提升吞吐量（🔶6-54、🔶6-60）。

#### （3）保障机制：动态状态管理避免空闲块耗尽

设计 4 种状态动态切换，通过 “软阈值（T_soft=5% 总块数）” 与 “硬阈值（T_hard）” 平衡 GC 与 I/O（🔶6-64、🔶6-70）：

|状态|行为|触发条件|
|---|---|---|
|S0|禁止 GC，仅处理 I/O|空闲块数 N_free > T_soft|
|S1|允许 GC + 所有 I/O 并行|T_hard < N_free < T_soft（核心并行状态）|
|S2|允许 GC + 禁止消耗空闲块的 I/O（如写）|N_free < T_hard|
|S3|仅执行 GC，禁止 I/O|高优先级写请求到来，需紧急回收空闲块|

### 2. 进阶方案：完全可抢占 GC（F-PGC）—— 硬件扩展的实时并行

#### （1）硬件基础：支持 “暂停 / 恢复” 命令的 NAND 闪存

传统闪存操作（读 / 写 / 擦除）是原子性的，F-PGC 通过硬件扩展，将操作拆分为 “脉冲 - 验证” 循环 phases（如 ISPP 编程：15V 脉冲→验证→0.5V 步进→成功），phase 间可暂停（开销仅 20μs），需 1 个额外页缓冲存储暂停操作的中间数据（🔶6-76、🔶6-81）。

#### （2）核心逻辑：任意操作阶段的 I/O 插入

突破 semi-PGC “固定抢占点” 限制，可在 GC 的任意操作（读 / 写 / 擦除）中间插入 I/O：

- 暂停当前 GC 操作，将中间数据存入额外页缓冲；
- 执行 I/O 请求（使用空闲页缓冲）；
- 恢复 GC 操作，从缓冲续接未完成的步骤（🔶6-88、🔶6-90）。
    
    例如：GC 执行块擦除时，若有写请求到达，可暂停擦除（20μs），处理写请求后恢复擦除，避免 1.5ms 的 I/O 阻塞。

### 3. 方案优势：无额外硬件成本与高兼容性

- semi-PGC 复用 SSD 现有页缓冲与命令逻辑，无硬件改动，计算开销仅 0.53%（每次页移动 2 次队列查询，耗时 1.2μs）（🔶6-47）；
- F-PGC 仅需 1 个额外页缓冲，暂停 / 恢复命令可基于现有闪存协议扩展（如 AMD 闪存的擦除暂停功能扩展至读写），兼容性强（🔶6-76、🔶6-182）。

## 三、实验验证与核心结果

论文基于 MSR SSD 模拟器（DiskSim 4.0 扩展），结合合成负载（请求大小 8-64KB、读写比 2:8）与真实负载（金融、Cello、TPC-H、OpenMail）验证方案有效性：

### 1. semi-PGC 性能

- 写主导负载（Cello）：平均响应时间降低 66.56%，响应时间方差降低 83.30%，最大响应时间降低 84.09%（🔶6-157、🔶6-159）；
- 混合读写负载（金融）：平均响应时间降低 6.05%，方差降低 49.82%（🔶6-157）；
- 资源利用率：通道利用率从传统 NPGC 的 37%-42% 提升至 68%-75%（🔶6-163）。

### 2. F-PGC 性能

- 较 semi-PGC 进一步提升：Cello 负载平均响应时间再降 14.57%，TPC-H 负载（读主导但含突发写）响应时间降低 68.13%（🔶6-183、🔶6-195）；
- 最坏情况响应时间（WCET）：从传统 NPGC 的 “T_er（1.5ms）+ max (U_er,U_ew)” 降至 “T_suspend（20μs）+ max (U_er,U_ew)”，稳定性显著提升（🔶6-111、🔶6-114）。

## 四、核心结论

论文通过 “半可抢占 GC + 完全可抢占 GC” 的分层方案，解决了传统 NPGC“GC 与 I/O 串行” 的固有缺陷：

1. 半可抢占 GC 通过抢占点与优化策略，实现低成本的 GC 与 I/O 并行，适配无特殊硬件的通用 SSD；
2. 完全可抢占 GC 通过硬件扩展，实现实时并行，满足低延迟、高突发负载场景需求；
3. 两种方案均复用 SSD 现有硬件特性，无显著成本，为企业级 SSD 的性能优化提供了可落地的技术路径。

**《Reducing SSD Read Latency via NAND Flash Program and Erase Suspension》**（2012，FAST）
td：redftl这些调度方案的学习，初步想法：根据请求写入数据的大小实现多个任务队列去做通道分配。